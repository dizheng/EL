<?xml version="1.0" encoding="UTF-8"?>
<LCTL_TEXT>
  <DOC id="ENG_NW_001027_20150224_F0000000X.xml" tokenization="tokenization_parameters" grammar="none" raw_text_char_length="5652" raw_text_md5="686cecb4005658260eec65de9b5cd4fd">
    <TEXT>
      <SEG id="segment-0" start_char="0" end_char="37">
        <ORIGINAL_TEXT>&lt;?xml version="1.0" encoding="utf-8"?&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-0-0" pos="unknown" morph="none" start_char="0" end_char="4">&lt;?xml</TOKEN>
        <TOKEN id="token-0-1" pos="unknown" morph="none" start_char="6" end_char="18">version="1.0"</TOKEN>
        <TOKEN id="token-0-2" pos="unknown" morph="none" start_char="20" end_char="37">encoding="utf-8"?&gt;</TOKEN>
      </SEG>
      <SEG id="segment-1" start_char="39" end_char="81">
        <ORIGINAL_TEXT>&lt;DOC id="ENG_NW_001027_20150224_F0000000X"&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-1-0" pos="unknown" morph="none" start_char="39" end_char="42">&lt;DOC</TOKEN>
        <TOKEN id="token-1-1" pos="unknown" morph="none" start_char="44" end_char="81">id="ENG_NW_001027_20150224_F0000000X"&gt;</TOKEN>
      </SEG>
      <SEG id="segment-2" start_char="83" end_char="188">
        <ORIGINAL_TEXT>&lt;SOURCE&gt;http://www.jpost.com/Israel-News/Analysis-So-what-if-Sara-Netanyahu-did-Bottlegate-391990&lt;/SOURCE&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-2-0" pos="unknown" morph="none" start_char="83" end_char="188">&lt;SOURCE&gt;http://www.jpost.com/Israel-News/Analysis-So-what-if-Sara-Netanyahu-did-Bottlegate-391990&lt;/SOURCE&gt;</TOKEN>
      </SEG>
      <SEG id="segment-3" start_char="190" end_char="231">
        <ORIGINAL_TEXT>&lt;DATE_TIME&gt;2015-02-24T00:00:00&lt;/DATE_TIME&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-3-0" pos="unknown" morph="none" start_char="190" end_char="231">&lt;DATE_TIME&gt;2015-02-24T00:00:00&lt;/DATE_TIME&gt;</TOKEN>
      </SEG>
      <SEG id="segment-4" start_char="233" end_char="242">
        <ORIGINAL_TEXT>&lt;HEADLINE&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-4-0" pos="unknown" morph="none" start_char="233" end_char="242">&lt;HEADLINE&gt;</TOKEN>
      </SEG>
      <SEG id="segment-5" start_char="244" end_char="294">
        <ORIGINAL_TEXT>Analysis: So what if Sara Netanyahu did Bottlegate?</ORIGINAL_TEXT>
        <TOKEN id="token-5-0" pos="word" morph="none" start_char="244" end_char="251">Analysis</TOKEN>
        <TOKEN id="token-5-1" pos="punct" morph="none" start_char="252" end_char="252">:</TOKEN>
        <TOKEN id="token-5-2" pos="word" morph="none" start_char="254" end_char="255">So</TOKEN>
        <TOKEN id="token-5-3" pos="word" morph="none" start_char="257" end_char="260">what</TOKEN>
        <TOKEN id="token-5-4" pos="word" morph="none" start_char="262" end_char="263">if</TOKEN>
        <TOKEN id="token-5-5" pos="word" morph="none" start_char="265" end_char="268">Sara</TOKEN>
        <TOKEN id="token-5-6" pos="word" morph="none" start_char="270" end_char="278">Netanyahu</TOKEN>
        <TOKEN id="token-5-7" pos="word" morph="none" start_char="280" end_char="282">did</TOKEN>
        <TOKEN id="token-5-8" pos="word" morph="none" start_char="284" end_char="293">Bottlegate</TOKEN>
        <TOKEN id="token-5-9" pos="punct" morph="none" start_char="294" end_char="294">?</TOKEN>
      </SEG>
      <SEG id="segment-6" start_char="296" end_char="306">
        <ORIGINAL_TEXT>&lt;/HEADLINE&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-6-0" pos="unknown" morph="none" start_char="296" end_char="306">&lt;/HEADLINE&gt;</TOKEN>
      </SEG>
      <SEG id="segment-7" start_char="308" end_char="313">
        <ORIGINAL_TEXT>&lt;TEXT&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-7-0" pos="unknown" morph="none" start_char="308" end_char="313">&lt;TEXT&gt;</TOKEN>
      </SEG>
      <SEG id="segment-8" start_char="315" end_char="317">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-8-0" pos="unknown" morph="none" start_char="315" end_char="317">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-9" start_char="319" end_char="371">
        <ORIGINAL_TEXT>Legally, the Bottlegate allegation may be a dead end.</ORIGINAL_TEXT>
        <TOKEN id="token-9-0" pos="word" morph="none" start_char="319" end_char="325">Legally</TOKEN>
        <TOKEN id="token-9-1" pos="punct" morph="none" start_char="326" end_char="326">,</TOKEN>
        <TOKEN id="token-9-2" pos="word" morph="none" start_char="328" end_char="330">the</TOKEN>
        <TOKEN id="token-9-3" pos="word" morph="none" start_char="332" end_char="341">Bottlegate</TOKEN>
        <TOKEN id="token-9-4" pos="word" morph="none" start_char="343" end_char="352">allegation</TOKEN>
        <TOKEN id="token-9-5" pos="word" morph="none" start_char="354" end_char="356">may</TOKEN>
        <TOKEN id="token-9-6" pos="word" morph="none" start_char="358" end_char="359">be</TOKEN>
        <TOKEN id="token-9-7" pos="word" morph="none" start_char="361" end_char="361">a</TOKEN>
        <TOKEN id="token-9-8" pos="word" morph="none" start_char="363" end_char="366">dead</TOKEN>
        <TOKEN id="token-9-9" pos="word" morph="none" start_char="368" end_char="370">end</TOKEN>
        <TOKEN id="token-9-10" pos="punct" morph="none" start_char="371" end_char="371">.</TOKEN>
      </SEG>
      <SEG id="segment-10" start_char="373" end_char="376">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-10-0" pos="unknown" morph="none" start_char="373" end_char="376">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-11" start_char="378" end_char="380">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-11-0" pos="unknown" morph="none" start_char="378" end_char="380">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-12" start_char="382" end_char="453">
        <ORIGINAL_TEXT>It is now quite possible that Sara Netanyahu did everything she has been</ORIGINAL_TEXT>
        <TOKEN id="token-12-0" pos="word" morph="none" start_char="382" end_char="383">It</TOKEN>
        <TOKEN id="token-12-1" pos="word" morph="none" start_char="385" end_char="386">is</TOKEN>
        <TOKEN id="token-12-2" pos="word" morph="none" start_char="388" end_char="390">now</TOKEN>
        <TOKEN id="token-12-3" pos="word" morph="none" start_char="392" end_char="396">quite</TOKEN>
        <TOKEN id="token-12-4" pos="word" morph="none" start_char="398" end_char="405">possible</TOKEN>
        <TOKEN id="token-12-5" pos="word" morph="none" start_char="407" end_char="410">that</TOKEN>
        <TOKEN id="token-12-6" pos="word" morph="none" start_char="412" end_char="415">Sara</TOKEN>
        <TOKEN id="token-12-7" pos="word" morph="none" start_char="417" end_char="425">Netanyahu</TOKEN>
        <TOKEN id="token-12-8" pos="word" morph="none" start_char="427" end_char="429">did</TOKEN>
        <TOKEN id="token-12-9" pos="word" morph="none" start_char="431" end_char="440">everything</TOKEN>
        <TOKEN id="token-12-10" pos="word" morph="none" start_char="442" end_char="444">she</TOKEN>
        <TOKEN id="token-12-11" pos="word" morph="none" start_char="446" end_char="448">has</TOKEN>
        <TOKEN id="token-12-12" pos="word" morph="none" start_char="450" end_char="453">been</TOKEN>
      </SEG>
      <SEG id="segment-13" start_char="455" end_char="525">
        <ORIGINAL_TEXT>accused of in “Bottlegate” and may still be legally bulletproof without</ORIGINAL_TEXT>
        <TOKEN id="token-13-0" pos="word" morph="none" start_char="455" end_char="461">accused</TOKEN>
        <TOKEN id="token-13-1" pos="word" morph="none" start_char="463" end_char="464">of</TOKEN>
        <TOKEN id="token-13-2" pos="word" morph="none" start_char="466" end_char="467">in</TOKEN>
        <TOKEN id="token-13-3" pos="punct" morph="none" start_char="469" end_char="469">“</TOKEN>
        <TOKEN id="token-13-4" pos="word" morph="none" start_char="470" end_char="479">Bottlegate</TOKEN>
        <TOKEN id="token-13-5" pos="punct" morph="none" start_char="480" end_char="480">”</TOKEN>
        <TOKEN id="token-13-6" pos="word" morph="none" start_char="482" end_char="484">and</TOKEN>
        <TOKEN id="token-13-7" pos="word" morph="none" start_char="486" end_char="488">may</TOKEN>
        <TOKEN id="token-13-8" pos="word" morph="none" start_char="490" end_char="494">still</TOKEN>
        <TOKEN id="token-13-9" pos="word" morph="none" start_char="496" end_char="497">be</TOKEN>
        <TOKEN id="token-13-10" pos="word" morph="none" start_char="499" end_char="505">legally</TOKEN>
        <TOKEN id="token-13-11" pos="word" morph="none" start_char="507" end_char="517">bulletproof</TOKEN>
        <TOKEN id="token-13-12" pos="word" morph="none" start_char="519" end_char="525">without</TOKEN>
      </SEG>
      <SEG id="segment-14" start_char="527" end_char="574">
        <ORIGINAL_TEXT>ever having to deny anything, or call a witness.</ORIGINAL_TEXT>
        <TOKEN id="token-14-0" pos="word" morph="none" start_char="527" end_char="530">ever</TOKEN>
        <TOKEN id="token-14-1" pos="word" morph="none" start_char="532" end_char="537">having</TOKEN>
        <TOKEN id="token-14-2" pos="word" morph="none" start_char="539" end_char="540">to</TOKEN>
        <TOKEN id="token-14-3" pos="word" morph="none" start_char="542" end_char="545">deny</TOKEN>
        <TOKEN id="token-14-4" pos="word" morph="none" start_char="547" end_char="554">anything</TOKEN>
        <TOKEN id="token-14-5" pos="punct" morph="none" start_char="555" end_char="555">,</TOKEN>
        <TOKEN id="token-14-6" pos="word" morph="none" start_char="557" end_char="558">or</TOKEN>
        <TOKEN id="token-14-7" pos="word" morph="none" start_char="560" end_char="563">call</TOKEN>
        <TOKEN id="token-14-8" pos="word" morph="none" start_char="565" end_char="565">a</TOKEN>
        <TOKEN id="token-14-9" pos="word" morph="none" start_char="567" end_char="573">witness</TOKEN>
        <TOKEN id="token-14-10" pos="punct" morph="none" start_char="574" end_char="574">.</TOKEN>
      </SEG>
      <SEG id="segment-15" start_char="576" end_char="579">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-15-0" pos="unknown" morph="none" start_char="576" end_char="579">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-16" start_char="581" end_char="583">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-16-0" pos="unknown" morph="none" start_char="581" end_char="583">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-17" start_char="585" end_char="658">
        <ORIGINAL_TEXT>Bottlegate refers to allegations that Sara Netanyahu, the prime minister’s</ORIGINAL_TEXT>
        <TOKEN id="token-17-0" pos="word" morph="none" start_char="585" end_char="594">Bottlegate</TOKEN>
        <TOKEN id="token-17-1" pos="word" morph="none" start_char="596" end_char="601">refers</TOKEN>
        <TOKEN id="token-17-2" pos="word" morph="none" start_char="603" end_char="604">to</TOKEN>
        <TOKEN id="token-17-3" pos="word" morph="none" start_char="606" end_char="616">allegations</TOKEN>
        <TOKEN id="token-17-4" pos="word" morph="none" start_char="618" end_char="621">that</TOKEN>
        <TOKEN id="token-17-5" pos="word" morph="none" start_char="623" end_char="626">Sara</TOKEN>
        <TOKEN id="token-17-6" pos="word" morph="none" start_char="628" end_char="636">Netanyahu</TOKEN>
        <TOKEN id="token-17-7" pos="punct" morph="none" start_char="637" end_char="637">,</TOKEN>
        <TOKEN id="token-17-8" pos="word" morph="none" start_char="639" end_char="641">the</TOKEN>
        <TOKEN id="token-17-9" pos="word" morph="none" start_char="643" end_char="647">prime</TOKEN>
        <TOKEN id="token-17-10" pos="word" morph="none" start_char="649" end_char="656">minister</TOKEN>
        <TOKEN id="token-17-11" pos="punct" morph="none" start_char="657" end_char="657">’</TOKEN>
        <TOKEN id="token-17-12" pos="word" morph="none" start_char="658" end_char="658">s</TOKEN>
      </SEG>
      <SEG id="segment-18" start_char="660" end_char="733">
        <ORIGINAL_TEXT>wife, and possibly others connected to her activities, may have improperly</ORIGINAL_TEXT>
        <TOKEN id="token-18-0" pos="word" morph="none" start_char="660" end_char="663">wife</TOKEN>
        <TOKEN id="token-18-1" pos="punct" morph="none" start_char="664" end_char="664">,</TOKEN>
        <TOKEN id="token-18-2" pos="word" morph="none" start_char="666" end_char="668">and</TOKEN>
        <TOKEN id="token-18-3" pos="word" morph="none" start_char="670" end_char="677">possibly</TOKEN>
        <TOKEN id="token-18-4" pos="word" morph="none" start_char="679" end_char="684">others</TOKEN>
        <TOKEN id="token-18-5" pos="word" morph="none" start_char="686" end_char="694">connected</TOKEN>
        <TOKEN id="token-18-6" pos="word" morph="none" start_char="696" end_char="697">to</TOKEN>
        <TOKEN id="token-18-7" pos="word" morph="none" start_char="699" end_char="701">her</TOKEN>
        <TOKEN id="token-18-8" pos="word" morph="none" start_char="703" end_char="712">activities</TOKEN>
        <TOKEN id="token-18-9" pos="punct" morph="none" start_char="713" end_char="713">,</TOKEN>
        <TOKEN id="token-18-10" pos="word" morph="none" start_char="715" end_char="717">may</TOKEN>
        <TOKEN id="token-18-11" pos="word" morph="none" start_char="719" end_char="722">have</TOKEN>
        <TOKEN id="token-18-12" pos="word" morph="none" start_char="724" end_char="733">improperly</TOKEN>
      </SEG>
      <SEG id="segment-19" start_char="735" end_char="810">
        <ORIGINAL_TEXT>pocketed NIS 4,000 cash from the collection of the deposit on recycled glass</ORIGINAL_TEXT>
        <TOKEN id="token-19-0" pos="word" morph="none" start_char="735" end_char="742">pocketed</TOKEN>
        <TOKEN id="token-19-1" pos="word" morph="none" start_char="744" end_char="746">NIS</TOKEN>
        <TOKEN id="token-19-2" pos="word" morph="none" start_char="748" end_char="748">4</TOKEN>
        <TOKEN id="token-19-3" pos="punct" morph="none" start_char="749" end_char="749">,</TOKEN>
        <TOKEN id="token-19-4" pos="word" morph="none" start_char="750" end_char="752">000</TOKEN>
        <TOKEN id="token-19-5" pos="word" morph="none" start_char="754" end_char="757">cash</TOKEN>
        <TOKEN id="token-19-6" pos="word" morph="none" start_char="759" end_char="762">from</TOKEN>
        <TOKEN id="token-19-7" pos="word" morph="none" start_char="764" end_char="766">the</TOKEN>
        <TOKEN id="token-19-8" pos="word" morph="none" start_char="768" end_char="777">collection</TOKEN>
        <TOKEN id="token-19-9" pos="word" morph="none" start_char="779" end_char="780">of</TOKEN>
        <TOKEN id="token-19-10" pos="word" morph="none" start_char="782" end_char="784">the</TOKEN>
        <TOKEN id="token-19-11" pos="word" morph="none" start_char="786" end_char="792">deposit</TOKEN>
        <TOKEN id="token-19-12" pos="word" morph="none" start_char="794" end_char="795">on</TOKEN>
        <TOKEN id="token-19-13" pos="word" morph="none" start_char="797" end_char="804">recycled</TOKEN>
        <TOKEN id="token-19-14" pos="word" morph="none" start_char="806" end_char="810">glass</TOKEN>
      </SEG>
      <SEG id="segment-20" start_char="812" end_char="868">
        <ORIGINAL_TEXT>bottles; though the bottles were bought with state funds.</ORIGINAL_TEXT>
        <TOKEN id="token-20-0" pos="word" morph="none" start_char="812" end_char="818">bottles</TOKEN>
        <TOKEN id="token-20-1" pos="punct" morph="none" start_char="819" end_char="819">;</TOKEN>
        <TOKEN id="token-20-2" pos="word" morph="none" start_char="821" end_char="826">though</TOKEN>
        <TOKEN id="token-20-3" pos="word" morph="none" start_char="828" end_char="830">the</TOKEN>
        <TOKEN id="token-20-4" pos="word" morph="none" start_char="832" end_char="838">bottles</TOKEN>
        <TOKEN id="token-20-5" pos="word" morph="none" start_char="840" end_char="843">were</TOKEN>
        <TOKEN id="token-20-6" pos="word" morph="none" start_char="845" end_char="850">bought</TOKEN>
        <TOKEN id="token-20-7" pos="word" morph="none" start_char="852" end_char="855">with</TOKEN>
        <TOKEN id="token-20-8" pos="word" morph="none" start_char="857" end_char="861">state</TOKEN>
        <TOKEN id="token-20-9" pos="word" morph="none" start_char="863" end_char="867">funds</TOKEN>
        <TOKEN id="token-20-10" pos="punct" morph="none" start_char="868" end_char="868">.</TOKEN>
      </SEG>
      <SEG id="segment-21" start_char="870" end_char="873">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-21-0" pos="unknown" morph="none" start_char="870" end_char="873">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-22" start_char="875" end_char="877">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-22-0" pos="unknown" morph="none" start_char="875" end_char="877">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-23" start_char="879" end_char="953">
        <ORIGINAL_TEXT>In Bottlegate’s early stages, there was controversy about whether the media</ORIGINAL_TEXT>
        <TOKEN id="token-23-0" pos="word" morph="none" start_char="879" end_char="880">In</TOKEN>
        <TOKEN id="token-23-1" pos="word" morph="none" start_char="882" end_char="891">Bottlegate</TOKEN>
        <TOKEN id="token-23-2" pos="punct" morph="none" start_char="892" end_char="892">’</TOKEN>
        <TOKEN id="token-23-3" pos="word" morph="none" start_char="893" end_char="893">s</TOKEN>
        <TOKEN id="token-23-4" pos="word" morph="none" start_char="895" end_char="899">early</TOKEN>
        <TOKEN id="token-23-5" pos="word" morph="none" start_char="901" end_char="906">stages</TOKEN>
        <TOKEN id="token-23-6" pos="punct" morph="none" start_char="907" end_char="907">,</TOKEN>
        <TOKEN id="token-23-7" pos="word" morph="none" start_char="909" end_char="913">there</TOKEN>
        <TOKEN id="token-23-8" pos="word" morph="none" start_char="915" end_char="917">was</TOKEN>
        <TOKEN id="token-23-9" pos="word" morph="none" start_char="919" end_char="929">controversy</TOKEN>
        <TOKEN id="token-23-10" pos="word" morph="none" start_char="931" end_char="935">about</TOKEN>
        <TOKEN id="token-23-11" pos="word" morph="none" start_char="937" end_char="943">whether</TOKEN>
        <TOKEN id="token-23-12" pos="word" morph="none" start_char="945" end_char="947">the</TOKEN>
        <TOKEN id="token-23-13" pos="word" morph="none" start_char="949" end_char="953">media</TOKEN>
      </SEG>
      <SEG id="segment-24" start_char="955" end_char="998">
        <ORIGINAL_TEXT>was inventing or exaggerating the situation.</ORIGINAL_TEXT>
        <TOKEN id="token-24-0" pos="word" morph="none" start_char="955" end_char="957">was</TOKEN>
        <TOKEN id="token-24-1" pos="word" morph="none" start_char="959" end_char="967">inventing</TOKEN>
        <TOKEN id="token-24-2" pos="word" morph="none" start_char="969" end_char="970">or</TOKEN>
        <TOKEN id="token-24-3" pos="word" morph="none" start_char="972" end_char="983">exaggerating</TOKEN>
        <TOKEN id="token-24-4" pos="word" morph="none" start_char="985" end_char="987">the</TOKEN>
        <TOKEN id="token-24-5" pos="word" morph="none" start_char="989" end_char="997">situation</TOKEN>
        <TOKEN id="token-24-6" pos="punct" morph="none" start_char="998" end_char="998">.</TOKEN>
      </SEG>
      <SEG id="segment-25" start_char="1000" end_char="1003">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-25-0" pos="unknown" morph="none" start_char="1000" end_char="1003">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-26" start_char="1005" end_char="1007">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-26-0" pos="unknown" morph="none" start_char="1005" end_char="1007">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-27" start_char="1009" end_char="1083">
        <ORIGINAL_TEXT>But as of last week’s state comptroller report, the basic above story about</ORIGINAL_TEXT>
        <TOKEN id="token-27-0" pos="word" morph="none" start_char="1009" end_char="1011">But</TOKEN>
        <TOKEN id="token-27-1" pos="word" morph="none" start_char="1013" end_char="1014">as</TOKEN>
        <TOKEN id="token-27-2" pos="word" morph="none" start_char="1016" end_char="1017">of</TOKEN>
        <TOKEN id="token-27-3" pos="word" morph="none" start_char="1019" end_char="1022">last</TOKEN>
        <TOKEN id="token-27-4" pos="word" morph="none" start_char="1024" end_char="1027">week</TOKEN>
        <TOKEN id="token-27-5" pos="punct" morph="none" start_char="1028" end_char="1028">’</TOKEN>
        <TOKEN id="token-27-6" pos="word" morph="none" start_char="1029" end_char="1029">s</TOKEN>
        <TOKEN id="token-27-7" pos="word" morph="none" start_char="1031" end_char="1035">state</TOKEN>
        <TOKEN id="token-27-8" pos="word" morph="none" start_char="1037" end_char="1047">comptroller</TOKEN>
        <TOKEN id="token-27-9" pos="word" morph="none" start_char="1049" end_char="1054">report</TOKEN>
        <TOKEN id="token-27-10" pos="punct" morph="none" start_char="1055" end_char="1055">,</TOKEN>
        <TOKEN id="token-27-11" pos="word" morph="none" start_char="1057" end_char="1059">the</TOKEN>
        <TOKEN id="token-27-12" pos="word" morph="none" start_char="1061" end_char="1065">basic</TOKEN>
        <TOKEN id="token-27-13" pos="word" morph="none" start_char="1067" end_char="1071">above</TOKEN>
        <TOKEN id="token-27-14" pos="word" morph="none" start_char="1073" end_char="1077">story</TOKEN>
        <TOKEN id="token-27-15" pos="word" morph="none" start_char="1079" end_char="1083">about</TOKEN>
      </SEG>
      <SEG id="segment-28" start_char="1085" end_char="1103">
        <ORIGINAL_TEXT>Sara was confirmed.</ORIGINAL_TEXT>
        <TOKEN id="token-28-0" pos="word" morph="none" start_char="1085" end_char="1088">Sara</TOKEN>
        <TOKEN id="token-28-1" pos="word" morph="none" start_char="1090" end_char="1092">was</TOKEN>
        <TOKEN id="token-28-2" pos="word" morph="none" start_char="1094" end_char="1102">confirmed</TOKEN>
        <TOKEN id="token-28-3" pos="punct" morph="none" start_char="1103" end_char="1103">.</TOKEN>
      </SEG>
      <SEG id="segment-29" start_char="1105" end_char="1108">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-29-0" pos="unknown" morph="none" start_char="1105" end_char="1108">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-30" start_char="1110" end_char="1112">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-30-0" pos="unknown" morph="none" start_char="1110" end_char="1112">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-31" start_char="1114" end_char="1188">
        <ORIGINAL_TEXT>State Comptroller Joseph Shapira also added that the “misuse of state funds</ORIGINAL_TEXT>
        <TOKEN id="token-31-0" pos="word" morph="none" start_char="1114" end_char="1118">State</TOKEN>
        <TOKEN id="token-31-1" pos="word" morph="none" start_char="1120" end_char="1130">Comptroller</TOKEN>
        <TOKEN id="token-31-2" pos="word" morph="none" start_char="1132" end_char="1137">Joseph</TOKEN>
        <TOKEN id="token-31-3" pos="word" morph="none" start_char="1139" end_char="1145">Shapira</TOKEN>
        <TOKEN id="token-31-4" pos="word" morph="none" start_char="1147" end_char="1150">also</TOKEN>
        <TOKEN id="token-31-5" pos="word" morph="none" start_char="1152" end_char="1156">added</TOKEN>
        <TOKEN id="token-31-6" pos="word" morph="none" start_char="1158" end_char="1161">that</TOKEN>
        <TOKEN id="token-31-7" pos="word" morph="none" start_char="1163" end_char="1165">the</TOKEN>
        <TOKEN id="token-31-8" pos="punct" morph="none" start_char="1167" end_char="1167">“</TOKEN>
        <TOKEN id="token-31-9" pos="word" morph="none" start_char="1168" end_char="1173">misuse</TOKEN>
        <TOKEN id="token-31-10" pos="word" morph="none" start_char="1175" end_char="1176">of</TOKEN>
        <TOKEN id="token-31-11" pos="word" morph="none" start_char="1178" end_char="1182">state</TOKEN>
        <TOKEN id="token-31-12" pos="word" morph="none" start_char="1184" end_char="1188">funds</TOKEN>
      </SEG>
      <SEG id="segment-32" start_char="1190" end_char="1265">
        <ORIGINAL_TEXT>must be treated with full severity no matter how small the amount,” and made</ORIGINAL_TEXT>
        <TOKEN id="token-32-0" pos="word" morph="none" start_char="1190" end_char="1193">must</TOKEN>
        <TOKEN id="token-32-1" pos="word" morph="none" start_char="1195" end_char="1196">be</TOKEN>
        <TOKEN id="token-32-2" pos="word" morph="none" start_char="1198" end_char="1204">treated</TOKEN>
        <TOKEN id="token-32-3" pos="word" morph="none" start_char="1206" end_char="1209">with</TOKEN>
        <TOKEN id="token-32-4" pos="word" morph="none" start_char="1211" end_char="1214">full</TOKEN>
        <TOKEN id="token-32-5" pos="word" morph="none" start_char="1216" end_char="1223">severity</TOKEN>
        <TOKEN id="token-32-6" pos="word" morph="none" start_char="1225" end_char="1226">no</TOKEN>
        <TOKEN id="token-32-7" pos="word" morph="none" start_char="1228" end_char="1233">matter</TOKEN>
        <TOKEN id="token-32-8" pos="word" morph="none" start_char="1235" end_char="1237">how</TOKEN>
        <TOKEN id="token-32-9" pos="word" morph="none" start_char="1239" end_char="1243">small</TOKEN>
        <TOKEN id="token-32-10" pos="word" morph="none" start_char="1245" end_char="1247">the</TOKEN>
        <TOKEN id="token-32-11" pos="word" morph="none" start_char="1249" end_char="1254">amount</TOKEN>
        <TOKEN id="token-32-12" pos="punct" morph="none" start_char="1255" end_char="1256">,”</TOKEN>
        <TOKEN id="token-32-13" pos="word" morph="none" start_char="1258" end_char="1260">and</TOKEN>
        <TOKEN id="token-32-14" pos="word" morph="none" start_char="1262" end_char="1265">made</TOKEN>
      </SEG>
      <SEG id="segment-33" start_char="1267" end_char="1342">
        <ORIGINAL_TEXT>reference that something about Bottlegate and some other allegations “raised</ORIGINAL_TEXT>
        <TOKEN id="token-33-0" pos="word" morph="none" start_char="1267" end_char="1275">reference</TOKEN>
        <TOKEN id="token-33-1" pos="word" morph="none" start_char="1277" end_char="1280">that</TOKEN>
        <TOKEN id="token-33-2" pos="word" morph="none" start_char="1282" end_char="1290">something</TOKEN>
        <TOKEN id="token-33-3" pos="word" morph="none" start_char="1292" end_char="1296">about</TOKEN>
        <TOKEN id="token-33-4" pos="word" morph="none" start_char="1298" end_char="1307">Bottlegate</TOKEN>
        <TOKEN id="token-33-5" pos="word" morph="none" start_char="1309" end_char="1311">and</TOKEN>
        <TOKEN id="token-33-6" pos="word" morph="none" start_char="1313" end_char="1316">some</TOKEN>
        <TOKEN id="token-33-7" pos="word" morph="none" start_char="1318" end_char="1322">other</TOKEN>
        <TOKEN id="token-33-8" pos="word" morph="none" start_char="1324" end_char="1334">allegations</TOKEN>
        <TOKEN id="token-33-9" pos="punct" morph="none" start_char="1336" end_char="1336">“</TOKEN>
        <TOKEN id="token-33-10" pos="word" morph="none" start_char="1337" end_char="1342">raised</TOKEN>
      </SEG>
      <SEG id="segment-34" start_char="1344" end_char="1421">
        <ORIGINAL_TEXT>prima facie suspicions of ethics violations, which could even raise suspicions</ORIGINAL_TEXT>
        <TOKEN id="token-34-0" pos="word" morph="none" start_char="1344" end_char="1348">prima</TOKEN>
        <TOKEN id="token-34-1" pos="word" morph="none" start_char="1350" end_char="1354">facie</TOKEN>
        <TOKEN id="token-34-2" pos="word" morph="none" start_char="1356" end_char="1365">suspicions</TOKEN>
        <TOKEN id="token-34-3" pos="word" morph="none" start_char="1367" end_char="1368">of</TOKEN>
        <TOKEN id="token-34-4" pos="word" morph="none" start_char="1370" end_char="1375">ethics</TOKEN>
        <TOKEN id="token-34-5" pos="word" morph="none" start_char="1377" end_char="1386">violations</TOKEN>
        <TOKEN id="token-34-6" pos="punct" morph="none" start_char="1387" end_char="1387">,</TOKEN>
        <TOKEN id="token-34-7" pos="word" morph="none" start_char="1389" end_char="1393">which</TOKEN>
        <TOKEN id="token-34-8" pos="word" morph="none" start_char="1395" end_char="1399">could</TOKEN>
        <TOKEN id="token-34-9" pos="word" morph="none" start_char="1401" end_char="1404">even</TOKEN>
        <TOKEN id="token-34-10" pos="word" morph="none" start_char="1406" end_char="1410">raise</TOKEN>
        <TOKEN id="token-34-11" pos="word" morph="none" start_char="1412" end_char="1421">suspicions</TOKEN>
      </SEG>
      <SEG id="segment-35" start_char="1423" end_char="1440">
        <ORIGINAL_TEXT>of criminal acts.”</ORIGINAL_TEXT>
        <TOKEN id="token-35-0" pos="word" morph="none" start_char="1423" end_char="1424">of</TOKEN>
        <TOKEN id="token-35-1" pos="word" morph="none" start_char="1426" end_char="1433">criminal</TOKEN>
        <TOKEN id="token-35-2" pos="word" morph="none" start_char="1435" end_char="1438">acts</TOKEN>
        <TOKEN id="token-35-3" pos="punct" morph="none" start_char="1439" end_char="1440">.”</TOKEN>
      </SEG>
      <SEG id="segment-36" start_char="1442" end_char="1445">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-36-0" pos="unknown" morph="none" start_char="1442" end_char="1445">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-37" start_char="1447" end_char="1449">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-37-0" pos="unknown" morph="none" start_char="1447" end_char="1449">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-38" start_char="1451" end_char="1520">
        <ORIGINAL_TEXT>Maybe there is even more to Bottlegate than we know, maybe Shapira was</ORIGINAL_TEXT>
        <TOKEN id="token-38-0" pos="word" morph="none" start_char="1451" end_char="1455">Maybe</TOKEN>
        <TOKEN id="token-38-1" pos="word" morph="none" start_char="1457" end_char="1461">there</TOKEN>
        <TOKEN id="token-38-2" pos="word" morph="none" start_char="1463" end_char="1464">is</TOKEN>
        <TOKEN id="token-38-3" pos="word" morph="none" start_char="1466" end_char="1469">even</TOKEN>
        <TOKEN id="token-38-4" pos="word" morph="none" start_char="1471" end_char="1474">more</TOKEN>
        <TOKEN id="token-38-5" pos="word" morph="none" start_char="1476" end_char="1477">to</TOKEN>
        <TOKEN id="token-38-6" pos="word" morph="none" start_char="1479" end_char="1488">Bottlegate</TOKEN>
        <TOKEN id="token-38-7" pos="word" morph="none" start_char="1490" end_char="1493">than</TOKEN>
        <TOKEN id="token-38-8" pos="word" morph="none" start_char="1495" end_char="1496">we</TOKEN>
        <TOKEN id="token-38-9" pos="word" morph="none" start_char="1498" end_char="1501">know</TOKEN>
        <TOKEN id="token-38-10" pos="punct" morph="none" start_char="1502" end_char="1502">,</TOKEN>
        <TOKEN id="token-38-11" pos="word" morph="none" start_char="1504" end_char="1508">maybe</TOKEN>
        <TOKEN id="token-38-12" pos="word" morph="none" start_char="1510" end_char="1516">Shapira</TOKEN>
        <TOKEN id="token-38-13" pos="word" morph="none" start_char="1518" end_char="1520">was</TOKEN>
      </SEG>
      <SEG id="segment-39" start_char="1522" end_char="1594">
        <ORIGINAL_TEXT>referring to obstruction of justice related to covering-up Bottlegate, or</ORIGINAL_TEXT>
        <TOKEN id="token-39-0" pos="word" morph="none" start_char="1522" end_char="1530">referring</TOKEN>
        <TOKEN id="token-39-1" pos="word" morph="none" start_char="1532" end_char="1533">to</TOKEN>
        <TOKEN id="token-39-2" pos="word" morph="none" start_char="1535" end_char="1545">obstruction</TOKEN>
        <TOKEN id="token-39-3" pos="word" morph="none" start_char="1547" end_char="1548">of</TOKEN>
        <TOKEN id="token-39-4" pos="word" morph="none" start_char="1550" end_char="1556">justice</TOKEN>
        <TOKEN id="token-39-5" pos="word" morph="none" start_char="1558" end_char="1564">related</TOKEN>
        <TOKEN id="token-39-6" pos="word" morph="none" start_char="1566" end_char="1567">to</TOKEN>
        <TOKEN id="token-39-7" pos="word" morph="none" start_char="1569" end_char="1576">covering</TOKEN>
        <TOKEN id="token-39-8" pos="punct" morph="none" start_char="1577" end_char="1577">-</TOKEN>
        <TOKEN id="token-39-9" pos="word" morph="none" start_char="1578" end_char="1579">up</TOKEN>
        <TOKEN id="token-39-10" pos="word" morph="none" start_char="1581" end_char="1590">Bottlegate</TOKEN>
        <TOKEN id="token-39-11" pos="punct" morph="none" start_char="1591" end_char="1591">,</TOKEN>
        <TOKEN id="token-39-12" pos="word" morph="none" start_char="1593" end_char="1594">or</TOKEN>
      </SEG>
      <SEG id="segment-40" start_char="1596" end_char="1666">
        <ORIGINAL_TEXT>maybe he was referring to a different piece of his report’s allegations</ORIGINAL_TEXT>
        <TOKEN id="token-40-0" pos="word" morph="none" start_char="1596" end_char="1600">maybe</TOKEN>
        <TOKEN id="token-40-1" pos="word" morph="none" start_char="1602" end_char="1603">he</TOKEN>
        <TOKEN id="token-40-2" pos="word" morph="none" start_char="1605" end_char="1607">was</TOKEN>
        <TOKEN id="token-40-3" pos="word" morph="none" start_char="1609" end_char="1617">referring</TOKEN>
        <TOKEN id="token-40-4" pos="word" morph="none" start_char="1619" end_char="1620">to</TOKEN>
        <TOKEN id="token-40-5" pos="word" morph="none" start_char="1622" end_char="1622">a</TOKEN>
        <TOKEN id="token-40-6" pos="word" morph="none" start_char="1624" end_char="1632">different</TOKEN>
        <TOKEN id="token-40-7" pos="word" morph="none" start_char="1634" end_char="1638">piece</TOKEN>
        <TOKEN id="token-40-8" pos="word" morph="none" start_char="1640" end_char="1641">of</TOKEN>
        <TOKEN id="token-40-9" pos="word" morph="none" start_char="1643" end_char="1645">his</TOKEN>
        <TOKEN id="token-40-10" pos="word" morph="none" start_char="1647" end_char="1652">report</TOKEN>
        <TOKEN id="token-40-11" pos="punct" morph="none" start_char="1653" end_char="1653">’</TOKEN>
        <TOKEN id="token-40-12" pos="word" morph="none" start_char="1654" end_char="1654">s</TOKEN>
        <TOKEN id="token-40-13" pos="word" morph="none" start_char="1656" end_char="1666">allegations</TOKEN>
      </SEG>
      <SEG id="segment-41" start_char="1668" end_char="1690">
        <ORIGINAL_TEXT>against the Netanyahus.</ORIGINAL_TEXT>
        <TOKEN id="token-41-0" pos="word" morph="none" start_char="1668" end_char="1674">against</TOKEN>
        <TOKEN id="token-41-1" pos="word" morph="none" start_char="1676" end_char="1678">the</TOKEN>
        <TOKEN id="token-41-2" pos="word" morph="none" start_char="1680" end_char="1689">Netanyahus</TOKEN>
        <TOKEN id="token-41-3" pos="punct" morph="none" start_char="1690" end_char="1690">.</TOKEN>
      </SEG>
      <SEG id="segment-42" start_char="1692" end_char="1695">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-42-0" pos="unknown" morph="none" start_char="1692" end_char="1695">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-43" start_char="1697" end_char="1699">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-43-0" pos="unknown" morph="none" start_char="1697" end_char="1699">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-44" start_char="1701" end_char="1774">
        <ORIGINAL_TEXT>But having spoken to an array of experts about the Bottlegate allegations,</ORIGINAL_TEXT>
        <TOKEN id="token-44-0" pos="word" morph="none" start_char="1701" end_char="1703">But</TOKEN>
        <TOKEN id="token-44-1" pos="word" morph="none" start_char="1705" end_char="1710">having</TOKEN>
        <TOKEN id="token-44-2" pos="word" morph="none" start_char="1712" end_char="1717">spoken</TOKEN>
        <TOKEN id="token-44-3" pos="word" morph="none" start_char="1719" end_char="1720">to</TOKEN>
        <TOKEN id="token-44-4" pos="word" morph="none" start_char="1722" end_char="1723">an</TOKEN>
        <TOKEN id="token-44-5" pos="word" morph="none" start_char="1725" end_char="1729">array</TOKEN>
        <TOKEN id="token-44-6" pos="word" morph="none" start_char="1731" end_char="1732">of</TOKEN>
        <TOKEN id="token-44-7" pos="word" morph="none" start_char="1734" end_char="1740">experts</TOKEN>
        <TOKEN id="token-44-8" pos="word" morph="none" start_char="1742" end_char="1746">about</TOKEN>
        <TOKEN id="token-44-9" pos="word" morph="none" start_char="1748" end_char="1750">the</TOKEN>
        <TOKEN id="token-44-10" pos="word" morph="none" start_char="1752" end_char="1761">Bottlegate</TOKEN>
        <TOKEN id="token-44-11" pos="word" morph="none" start_char="1763" end_char="1773">allegations</TOKEN>
        <TOKEN id="token-44-12" pos="punct" morph="none" start_char="1774" end_char="1774">,</TOKEN>
      </SEG>
      <SEG id="segment-45" start_char="1776" end_char="1844">
        <ORIGINAL_TEXT>there is a definite consensus that regarding the legal perspective to</ORIGINAL_TEXT>
        <TOKEN id="token-45-0" pos="word" morph="none" start_char="1776" end_char="1780">there</TOKEN>
        <TOKEN id="token-45-1" pos="word" morph="none" start_char="1782" end_char="1783">is</TOKEN>
        <TOKEN id="token-45-2" pos="word" morph="none" start_char="1785" end_char="1785">a</TOKEN>
        <TOKEN id="token-45-3" pos="word" morph="none" start_char="1787" end_char="1794">definite</TOKEN>
        <TOKEN id="token-45-4" pos="word" morph="none" start_char="1796" end_char="1804">consensus</TOKEN>
        <TOKEN id="token-45-5" pos="word" morph="none" start_char="1806" end_char="1809">that</TOKEN>
        <TOKEN id="token-45-6" pos="word" morph="none" start_char="1811" end_char="1819">regarding</TOKEN>
        <TOKEN id="token-45-7" pos="word" morph="none" start_char="1821" end_char="1823">the</TOKEN>
        <TOKEN id="token-45-8" pos="word" morph="none" start_char="1825" end_char="1829">legal</TOKEN>
        <TOKEN id="token-45-9" pos="word" morph="none" start_char="1831" end_char="1841">perspective</TOKEN>
        <TOKEN id="token-45-10" pos="word" morph="none" start_char="1843" end_char="1844">to</TOKEN>
      </SEG>
      <SEG id="segment-46" start_char="1846" end_char="1910">
        <ORIGINAL_TEXT>Bottlegate, Sara is “indictment proof” from any criminal offense,</ORIGINAL_TEXT>
        <TOKEN id="token-46-0" pos="word" morph="none" start_char="1846" end_char="1855">Bottlegate</TOKEN>
        <TOKEN id="token-46-1" pos="punct" morph="none" start_char="1856" end_char="1856">,</TOKEN>
        <TOKEN id="token-46-2" pos="word" morph="none" start_char="1858" end_char="1861">Sara</TOKEN>
        <TOKEN id="token-46-3" pos="word" morph="none" start_char="1863" end_char="1864">is</TOKEN>
        <TOKEN id="token-46-4" pos="punct" morph="none" start_char="1866" end_char="1866">“</TOKEN>
        <TOKEN id="token-46-5" pos="word" morph="none" start_char="1867" end_char="1876">indictment</TOKEN>
        <TOKEN id="token-46-6" pos="word" morph="none" start_char="1878" end_char="1882">proof</TOKEN>
        <TOKEN id="token-46-7" pos="punct" morph="none" start_char="1883" end_char="1883">”</TOKEN>
        <TOKEN id="token-46-8" pos="word" morph="none" start_char="1885" end_char="1888">from</TOKEN>
        <TOKEN id="token-46-9" pos="word" morph="none" start_char="1890" end_char="1892">any</TOKEN>
        <TOKEN id="token-46-10" pos="word" morph="none" start_char="1894" end_char="1901">criminal</TOKEN>
        <TOKEN id="token-46-11" pos="word" morph="none" start_char="1903" end_char="1909">offense</TOKEN>
        <TOKEN id="token-46-12" pos="punct" morph="none" start_char="1910" end_char="1910">,</TOKEN>
      </SEG>
      <SEG id="segment-47" start_char="1912" end_char="1943">
        <ORIGINAL_TEXT>regardless of what Shapira says.</ORIGINAL_TEXT>
        <TOKEN id="token-47-0" pos="word" morph="none" start_char="1912" end_char="1921">regardless</TOKEN>
        <TOKEN id="token-47-1" pos="word" morph="none" start_char="1923" end_char="1924">of</TOKEN>
        <TOKEN id="token-47-2" pos="word" morph="none" start_char="1926" end_char="1929">what</TOKEN>
        <TOKEN id="token-47-3" pos="word" morph="none" start_char="1931" end_char="1937">Shapira</TOKEN>
        <TOKEN id="token-47-4" pos="word" morph="none" start_char="1939" end_char="1942">says</TOKEN>
        <TOKEN id="token-47-5" pos="punct" morph="none" start_char="1943" end_char="1943">.</TOKEN>
      </SEG>
      <SEG id="segment-48" start_char="1945" end_char="1948">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-48-0" pos="unknown" morph="none" start_char="1945" end_char="1948">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-49" start_char="1950" end_char="1952">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-49-0" pos="unknown" morph="none" start_char="1950" end_char="1952">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-50" start_char="1954" end_char="2028">
        <ORIGINAL_TEXT>Netanyahu’s lawyer, Jacob Weinroth, was the first to raise the defense that</ORIGINAL_TEXT>
        <TOKEN id="token-50-0" pos="word" morph="none" start_char="1954" end_char="1962">Netanyahu</TOKEN>
        <TOKEN id="token-50-1" pos="punct" morph="none" start_char="1963" end_char="1963">’</TOKEN>
        <TOKEN id="token-50-2" pos="word" morph="none" start_char="1964" end_char="1964">s</TOKEN>
        <TOKEN id="token-50-3" pos="word" morph="none" start_char="1966" end_char="1971">lawyer</TOKEN>
        <TOKEN id="token-50-4" pos="punct" morph="none" start_char="1972" end_char="1972">,</TOKEN>
        <TOKEN id="token-50-5" pos="word" morph="none" start_char="1974" end_char="1978">Jacob</TOKEN>
        <TOKEN id="token-50-6" pos="word" morph="none" start_char="1980" end_char="1987">Weinroth</TOKEN>
        <TOKEN id="token-50-7" pos="punct" morph="none" start_char="1988" end_char="1988">,</TOKEN>
        <TOKEN id="token-50-8" pos="word" morph="none" start_char="1990" end_char="1992">was</TOKEN>
        <TOKEN id="token-50-9" pos="word" morph="none" start_char="1994" end_char="1996">the</TOKEN>
        <TOKEN id="token-50-10" pos="word" morph="none" start_char="1998" end_char="2002">first</TOKEN>
        <TOKEN id="token-50-11" pos="word" morph="none" start_char="2004" end_char="2005">to</TOKEN>
        <TOKEN id="token-50-12" pos="word" morph="none" start_char="2007" end_char="2011">raise</TOKEN>
        <TOKEN id="token-50-13" pos="word" morph="none" start_char="2013" end_char="2015">the</TOKEN>
        <TOKEN id="token-50-14" pos="word" morph="none" start_char="2017" end_char="2023">defense</TOKEN>
        <TOKEN id="token-50-15" pos="word" morph="none" start_char="2025" end_char="2028">that</TOKEN>
      </SEG>
      <SEG id="segment-51" start_char="2030" end_char="2107">
        <ORIGINAL_TEXT>it can be irrelevant, under the law regarding returning bottles for receipt of</ORIGINAL_TEXT>
        <TOKEN id="token-51-0" pos="word" morph="none" start_char="2030" end_char="2031">it</TOKEN>
        <TOKEN id="token-51-1" pos="word" morph="none" start_char="2033" end_char="2035">can</TOKEN>
        <TOKEN id="token-51-2" pos="word" morph="none" start_char="2037" end_char="2038">be</TOKEN>
        <TOKEN id="token-51-3" pos="word" morph="none" start_char="2040" end_char="2049">irrelevant</TOKEN>
        <TOKEN id="token-51-4" pos="punct" morph="none" start_char="2050" end_char="2050">,</TOKEN>
        <TOKEN id="token-51-5" pos="word" morph="none" start_char="2052" end_char="2056">under</TOKEN>
        <TOKEN id="token-51-6" pos="word" morph="none" start_char="2058" end_char="2060">the</TOKEN>
        <TOKEN id="token-51-7" pos="word" morph="none" start_char="2062" end_char="2064">law</TOKEN>
        <TOKEN id="token-51-8" pos="word" morph="none" start_char="2066" end_char="2074">regarding</TOKEN>
        <TOKEN id="token-51-9" pos="word" morph="none" start_char="2076" end_char="2084">returning</TOKEN>
        <TOKEN id="token-51-10" pos="word" morph="none" start_char="2086" end_char="2092">bottles</TOKEN>
        <TOKEN id="token-51-11" pos="word" morph="none" start_char="2094" end_char="2096">for</TOKEN>
        <TOKEN id="token-51-12" pos="word" morph="none" start_char="2098" end_char="2104">receipt</TOKEN>
        <TOKEN id="token-51-13" pos="word" morph="none" start_char="2106" end_char="2107">of</TOKEN>
      </SEG>
      <SEG id="segment-52" start_char="2109" end_char="2182">
        <ORIGINAL_TEXT>funds, who bought the bottles – as both the bottle buyer and collector who</ORIGINAL_TEXT>
        <TOKEN id="token-52-0" pos="word" morph="none" start_char="2109" end_char="2113">funds</TOKEN>
        <TOKEN id="token-52-1" pos="punct" morph="none" start_char="2114" end_char="2114">,</TOKEN>
        <TOKEN id="token-52-2" pos="word" morph="none" start_char="2116" end_char="2118">who</TOKEN>
        <TOKEN id="token-52-3" pos="word" morph="none" start_char="2120" end_char="2125">bought</TOKEN>
        <TOKEN id="token-52-4" pos="word" morph="none" start_char="2127" end_char="2129">the</TOKEN>
        <TOKEN id="token-52-5" pos="word" morph="none" start_char="2131" end_char="2137">bottles</TOKEN>
        <TOKEN id="token-52-6" pos="unknown" morph="none" start_char="2139" end_char="2139">–</TOKEN>
        <TOKEN id="token-52-7" pos="word" morph="none" start_char="2141" end_char="2142">as</TOKEN>
        <TOKEN id="token-52-8" pos="word" morph="none" start_char="2144" end_char="2147">both</TOKEN>
        <TOKEN id="token-52-9" pos="word" morph="none" start_char="2149" end_char="2151">the</TOKEN>
        <TOKEN id="token-52-10" pos="word" morph="none" start_char="2153" end_char="2158">bottle</TOKEN>
        <TOKEN id="token-52-11" pos="word" morph="none" start_char="2160" end_char="2164">buyer</TOKEN>
        <TOKEN id="token-52-12" pos="word" morph="none" start_char="2166" end_char="2168">and</TOKEN>
        <TOKEN id="token-52-13" pos="word" morph="none" start_char="2170" end_char="2178">collector</TOKEN>
        <TOKEN id="token-52-14" pos="word" morph="none" start_char="2180" end_char="2182">who</TOKEN>
      </SEG>
      <SEG id="segment-53" start_char="2184" end_char="2238">
        <ORIGINAL_TEXT>turns them in are legally entitled to receipt of funds.</ORIGINAL_TEXT>
        <TOKEN id="token-53-0" pos="word" morph="none" start_char="2184" end_char="2188">turns</TOKEN>
        <TOKEN id="token-53-1" pos="word" morph="none" start_char="2190" end_char="2193">them</TOKEN>
        <TOKEN id="token-53-2" pos="word" morph="none" start_char="2195" end_char="2196">in</TOKEN>
        <TOKEN id="token-53-3" pos="word" morph="none" start_char="2198" end_char="2200">are</TOKEN>
        <TOKEN id="token-53-4" pos="word" morph="none" start_char="2202" end_char="2208">legally</TOKEN>
        <TOKEN id="token-53-5" pos="word" morph="none" start_char="2210" end_char="2217">entitled</TOKEN>
        <TOKEN id="token-53-6" pos="word" morph="none" start_char="2219" end_char="2220">to</TOKEN>
        <TOKEN id="token-53-7" pos="word" morph="none" start_char="2222" end_char="2228">receipt</TOKEN>
        <TOKEN id="token-53-8" pos="word" morph="none" start_char="2230" end_char="2231">of</TOKEN>
        <TOKEN id="token-53-9" pos="word" morph="none" start_char="2233" end_char="2237">funds</TOKEN>
        <TOKEN id="token-53-10" pos="punct" morph="none" start_char="2238" end_char="2238">.</TOKEN>
      </SEG>
      <SEG id="segment-54" start_char="2240" end_char="2243">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-54-0" pos="unknown" morph="none" start_char="2240" end_char="2243">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-55" start_char="2245" end_char="2247">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-55-0" pos="unknown" morph="none" start_char="2245" end_char="2247">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-56" start_char="2249" end_char="2313">
        <ORIGINAL_TEXT>Meaning Sara was due the money since she “collected” the bottles.</ORIGINAL_TEXT>
        <TOKEN id="token-56-0" pos="word" morph="none" start_char="2249" end_char="2255">Meaning</TOKEN>
        <TOKEN id="token-56-1" pos="word" morph="none" start_char="2257" end_char="2260">Sara</TOKEN>
        <TOKEN id="token-56-2" pos="word" morph="none" start_char="2262" end_char="2264">was</TOKEN>
        <TOKEN id="token-56-3" pos="word" morph="none" start_char="2266" end_char="2268">due</TOKEN>
        <TOKEN id="token-56-4" pos="word" morph="none" start_char="2270" end_char="2272">the</TOKEN>
        <TOKEN id="token-56-5" pos="word" morph="none" start_char="2274" end_char="2278">money</TOKEN>
        <TOKEN id="token-56-6" pos="word" morph="none" start_char="2280" end_char="2284">since</TOKEN>
        <TOKEN id="token-56-7" pos="word" morph="none" start_char="2286" end_char="2288">she</TOKEN>
        <TOKEN id="token-56-8" pos="punct" morph="none" start_char="2290" end_char="2290">“</TOKEN>
        <TOKEN id="token-56-9" pos="word" morph="none" start_char="2291" end_char="2299">collected</TOKEN>
        <TOKEN id="token-56-10" pos="punct" morph="none" start_char="2300" end_char="2300">”</TOKEN>
        <TOKEN id="token-56-11" pos="word" morph="none" start_char="2302" end_char="2304">the</TOKEN>
        <TOKEN id="token-56-12" pos="word" morph="none" start_char="2306" end_char="2312">bottles</TOKEN>
        <TOKEN id="token-56-13" pos="punct" morph="none" start_char="2313" end_char="2313">.</TOKEN>
      </SEG>
      <SEG id="segment-57" start_char="2315" end_char="2318">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-57-0" pos="unknown" morph="none" start_char="2315" end_char="2318">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-58" start_char="2320" end_char="2322">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-58-0" pos="unknown" morph="none" start_char="2320" end_char="2322">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-59" start_char="2324" end_char="2401">
        <ORIGINAL_TEXT>Shmuel Saadia, a lawyer, author and expert in criminal law dealing with public</ORIGINAL_TEXT>
        <TOKEN id="token-59-0" pos="word" morph="none" start_char="2324" end_char="2329">Shmuel</TOKEN>
        <TOKEN id="token-59-1" pos="word" morph="none" start_char="2331" end_char="2336">Saadia</TOKEN>
        <TOKEN id="token-59-2" pos="punct" morph="none" start_char="2337" end_char="2337">,</TOKEN>
        <TOKEN id="token-59-3" pos="word" morph="none" start_char="2339" end_char="2339">a</TOKEN>
        <TOKEN id="token-59-4" pos="word" morph="none" start_char="2341" end_char="2346">lawyer</TOKEN>
        <TOKEN id="token-59-5" pos="punct" morph="none" start_char="2347" end_char="2347">,</TOKEN>
        <TOKEN id="token-59-6" pos="word" morph="none" start_char="2349" end_char="2354">author</TOKEN>
        <TOKEN id="token-59-7" pos="word" morph="none" start_char="2356" end_char="2358">and</TOKEN>
        <TOKEN id="token-59-8" pos="word" morph="none" start_char="2360" end_char="2365">expert</TOKEN>
        <TOKEN id="token-59-9" pos="word" morph="none" start_char="2367" end_char="2368">in</TOKEN>
        <TOKEN id="token-59-10" pos="word" morph="none" start_char="2370" end_char="2377">criminal</TOKEN>
        <TOKEN id="token-59-11" pos="word" morph="none" start_char="2379" end_char="2381">law</TOKEN>
        <TOKEN id="token-59-12" pos="word" morph="none" start_char="2383" end_char="2389">dealing</TOKEN>
        <TOKEN id="token-59-13" pos="word" morph="none" start_char="2391" end_char="2394">with</TOKEN>
        <TOKEN id="token-59-14" pos="word" morph="none" start_char="2396" end_char="2401">public</TOKEN>
      </SEG>
      <SEG id="segment-60" start_char="2403" end_char="2476">
        <ORIGINAL_TEXT>officials, said this argument makes Sara’s actions and intent “free of any</ORIGINAL_TEXT>
        <TOKEN id="token-60-0" pos="word" morph="none" start_char="2403" end_char="2411">officials</TOKEN>
        <TOKEN id="token-60-1" pos="punct" morph="none" start_char="2412" end_char="2412">,</TOKEN>
        <TOKEN id="token-60-2" pos="word" morph="none" start_char="2414" end_char="2417">said</TOKEN>
        <TOKEN id="token-60-3" pos="word" morph="none" start_char="2419" end_char="2422">this</TOKEN>
        <TOKEN id="token-60-4" pos="word" morph="none" start_char="2424" end_char="2431">argument</TOKEN>
        <TOKEN id="token-60-5" pos="word" morph="none" start_char="2433" end_char="2437">makes</TOKEN>
        <TOKEN id="token-60-6" pos="word" morph="none" start_char="2439" end_char="2442">Sara</TOKEN>
        <TOKEN id="token-60-7" pos="punct" morph="none" start_char="2443" end_char="2443">’</TOKEN>
        <TOKEN id="token-60-8" pos="word" morph="none" start_char="2444" end_char="2444">s</TOKEN>
        <TOKEN id="token-60-9" pos="word" morph="none" start_char="2446" end_char="2452">actions</TOKEN>
        <TOKEN id="token-60-10" pos="word" morph="none" start_char="2454" end_char="2456">and</TOKEN>
        <TOKEN id="token-60-11" pos="word" morph="none" start_char="2458" end_char="2463">intent</TOKEN>
        <TOKEN id="token-60-12" pos="punct" morph="none" start_char="2465" end_char="2465">“</TOKEN>
        <TOKEN id="token-60-13" pos="word" morph="none" start_char="2466" end_char="2469">free</TOKEN>
        <TOKEN id="token-60-14" pos="word" morph="none" start_char="2471" end_char="2472">of</TOKEN>
        <TOKEN id="token-60-15" pos="word" morph="none" start_char="2474" end_char="2476">any</TOKEN>
      </SEG>
      <SEG id="segment-61" start_char="2478" end_char="2495">
        <ORIGINAL_TEXT>criminal aspects.”</ORIGINAL_TEXT>
        <TOKEN id="token-61-0" pos="word" morph="none" start_char="2478" end_char="2485">criminal</TOKEN>
        <TOKEN id="token-61-1" pos="word" morph="none" start_char="2487" end_char="2493">aspects</TOKEN>
        <TOKEN id="token-61-2" pos="punct" morph="none" start_char="2494" end_char="2495">.”</TOKEN>
      </SEG>
      <SEG id="segment-62" start_char="2497" end_char="2500">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-62-0" pos="unknown" morph="none" start_char="2497" end_char="2500">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-63" start_char="2502" end_char="2504">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-63-0" pos="unknown" morph="none" start_char="2502" end_char="2504">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-64" start_char="2506" end_char="2577">
        <ORIGINAL_TEXT>Saadia noted that “there is no obligation for individuals to return” the</ORIGINAL_TEXT>
        <TOKEN id="token-64-0" pos="word" morph="none" start_char="2506" end_char="2511">Saadia</TOKEN>
        <TOKEN id="token-64-1" pos="word" morph="none" start_char="2513" end_char="2517">noted</TOKEN>
        <TOKEN id="token-64-2" pos="word" morph="none" start_char="2519" end_char="2522">that</TOKEN>
        <TOKEN id="token-64-3" pos="punct" morph="none" start_char="2524" end_char="2524">“</TOKEN>
        <TOKEN id="token-64-4" pos="word" morph="none" start_char="2525" end_char="2529">there</TOKEN>
        <TOKEN id="token-64-5" pos="word" morph="none" start_char="2531" end_char="2532">is</TOKEN>
        <TOKEN id="token-64-6" pos="word" morph="none" start_char="2534" end_char="2535">no</TOKEN>
        <TOKEN id="token-64-7" pos="word" morph="none" start_char="2537" end_char="2546">obligation</TOKEN>
        <TOKEN id="token-64-8" pos="word" morph="none" start_char="2548" end_char="2550">for</TOKEN>
        <TOKEN id="token-64-9" pos="word" morph="none" start_char="2552" end_char="2562">individuals</TOKEN>
        <TOKEN id="token-64-10" pos="word" morph="none" start_char="2564" end_char="2565">to</TOKEN>
        <TOKEN id="token-64-11" pos="word" morph="none" start_char="2567" end_char="2572">return</TOKEN>
        <TOKEN id="token-64-12" pos="punct" morph="none" start_char="2573" end_char="2573">”</TOKEN>
        <TOKEN id="token-64-13" pos="word" morph="none" start_char="2575" end_char="2577">the</TOKEN>
      </SEG>
      <SEG id="segment-65" start_char="2579" end_char="2652">
        <ORIGINAL_TEXT>bottles in the first place, so there can be no crime related to who or how</ORIGINAL_TEXT>
        <TOKEN id="token-65-0" pos="word" morph="none" start_char="2579" end_char="2585">bottles</TOKEN>
        <TOKEN id="token-65-1" pos="word" morph="none" start_char="2587" end_char="2588">in</TOKEN>
        <TOKEN id="token-65-2" pos="word" morph="none" start_char="2590" end_char="2592">the</TOKEN>
        <TOKEN id="token-65-3" pos="word" morph="none" start_char="2594" end_char="2598">first</TOKEN>
        <TOKEN id="token-65-4" pos="word" morph="none" start_char="2600" end_char="2604">place</TOKEN>
        <TOKEN id="token-65-5" pos="punct" morph="none" start_char="2605" end_char="2605">,</TOKEN>
        <TOKEN id="token-65-6" pos="word" morph="none" start_char="2607" end_char="2608">so</TOKEN>
        <TOKEN id="token-65-7" pos="word" morph="none" start_char="2610" end_char="2614">there</TOKEN>
        <TOKEN id="token-65-8" pos="word" morph="none" start_char="2616" end_char="2618">can</TOKEN>
        <TOKEN id="token-65-9" pos="word" morph="none" start_char="2620" end_char="2621">be</TOKEN>
        <TOKEN id="token-65-10" pos="word" morph="none" start_char="2623" end_char="2624">no</TOKEN>
        <TOKEN id="token-65-11" pos="word" morph="none" start_char="2626" end_char="2630">crime</TOKEN>
        <TOKEN id="token-65-12" pos="word" morph="none" start_char="2632" end_char="2638">related</TOKEN>
        <TOKEN id="token-65-13" pos="word" morph="none" start_char="2640" end_char="2641">to</TOKEN>
        <TOKEN id="token-65-14" pos="word" morph="none" start_char="2643" end_char="2645">who</TOKEN>
        <TOKEN id="token-65-15" pos="word" morph="none" start_char="2647" end_char="2648">or</TOKEN>
        <TOKEN id="token-65-16" pos="word" morph="none" start_char="2650" end_char="2652">how</TOKEN>
      </SEG>
      <SEG id="segment-66" start_char="2654" end_char="2671">
        <ORIGINAL_TEXT>they are returned.</ORIGINAL_TEXT>
        <TOKEN id="token-66-0" pos="word" morph="none" start_char="2654" end_char="2657">they</TOKEN>
        <TOKEN id="token-66-1" pos="word" morph="none" start_char="2659" end_char="2661">are</TOKEN>
        <TOKEN id="token-66-2" pos="word" morph="none" start_char="2663" end_char="2670">returned</TOKEN>
        <TOKEN id="token-66-3" pos="punct" morph="none" start_char="2671" end_char="2671">.</TOKEN>
      </SEG>
      <SEG id="segment-67" start_char="2673" end_char="2676">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-67-0" pos="unknown" morph="none" start_char="2673" end_char="2676">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-68" start_char="2678" end_char="2680">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-68-0" pos="unknown" morph="none" start_char="2678" end_char="2680">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-69" start_char="2682" end_char="2753">
        <ORIGINAL_TEXT>In the worst case scenario, he said that even if someone made “an absurd</ORIGINAL_TEXT>
        <TOKEN id="token-69-0" pos="word" morph="none" start_char="2682" end_char="2683">In</TOKEN>
        <TOKEN id="token-69-1" pos="word" morph="none" start_char="2685" end_char="2687">the</TOKEN>
        <TOKEN id="token-69-2" pos="word" morph="none" start_char="2689" end_char="2693">worst</TOKEN>
        <TOKEN id="token-69-3" pos="word" morph="none" start_char="2695" end_char="2698">case</TOKEN>
        <TOKEN id="token-69-4" pos="word" morph="none" start_char="2700" end_char="2707">scenario</TOKEN>
        <TOKEN id="token-69-5" pos="punct" morph="none" start_char="2708" end_char="2708">,</TOKEN>
        <TOKEN id="token-69-6" pos="word" morph="none" start_char="2710" end_char="2711">he</TOKEN>
        <TOKEN id="token-69-7" pos="word" morph="none" start_char="2713" end_char="2716">said</TOKEN>
        <TOKEN id="token-69-8" pos="word" morph="none" start_char="2718" end_char="2721">that</TOKEN>
        <TOKEN id="token-69-9" pos="word" morph="none" start_char="2723" end_char="2726">even</TOKEN>
        <TOKEN id="token-69-10" pos="word" morph="none" start_char="2728" end_char="2729">if</TOKEN>
        <TOKEN id="token-69-11" pos="word" morph="none" start_char="2731" end_char="2737">someone</TOKEN>
        <TOKEN id="token-69-12" pos="word" morph="none" start_char="2739" end_char="2742">made</TOKEN>
        <TOKEN id="token-69-13" pos="punct" morph="none" start_char="2744" end_char="2744">“</TOKEN>
        <TOKEN id="token-69-14" pos="word" morph="none" start_char="2745" end_char="2746">an</TOKEN>
        <TOKEN id="token-69-15" pos="word" morph="none" start_char="2748" end_char="2753">absurd</TOKEN>
      </SEG>
      <SEG id="segment-70" start_char="2755" end_char="2825">
        <ORIGINAL_TEXT>interpretation” of the law to try to ensnare Sara, at worst she made or</ORIGINAL_TEXT>
        <TOKEN id="token-70-0" pos="word" morph="none" start_char="2755" end_char="2768">interpretation</TOKEN>
        <TOKEN id="token-70-1" pos="punct" morph="none" start_char="2769" end_char="2769">”</TOKEN>
        <TOKEN id="token-70-2" pos="word" morph="none" start_char="2771" end_char="2772">of</TOKEN>
        <TOKEN id="token-70-3" pos="word" morph="none" start_char="2774" end_char="2776">the</TOKEN>
        <TOKEN id="token-70-4" pos="word" morph="none" start_char="2778" end_char="2780">law</TOKEN>
        <TOKEN id="token-70-5" pos="word" morph="none" start_char="2782" end_char="2783">to</TOKEN>
        <TOKEN id="token-70-6" pos="word" morph="none" start_char="2785" end_char="2787">try</TOKEN>
        <TOKEN id="token-70-7" pos="word" morph="none" start_char="2789" end_char="2790">to</TOKEN>
        <TOKEN id="token-70-8" pos="word" morph="none" start_char="2792" end_char="2798">ensnare</TOKEN>
        <TOKEN id="token-70-9" pos="word" morph="none" start_char="2800" end_char="2803">Sara</TOKEN>
        <TOKEN id="token-70-10" pos="punct" morph="none" start_char="2804" end_char="2804">,</TOKEN>
        <TOKEN id="token-70-11" pos="word" morph="none" start_char="2806" end_char="2807">at</TOKEN>
        <TOKEN id="token-70-12" pos="word" morph="none" start_char="2809" end_char="2813">worst</TOKEN>
        <TOKEN id="token-70-13" pos="word" morph="none" start_char="2815" end_char="2817">she</TOKEN>
        <TOKEN id="token-70-14" pos="word" morph="none" start_char="2819" end_char="2822">made</TOKEN>
        <TOKEN id="token-70-15" pos="word" morph="none" start_char="2824" end_char="2825">or</TOKEN>
      </SEG>
      <SEG id="segment-71" start_char="2827" end_char="2874">
        <ORIGINAL_TEXT>could claim to have “made a good faith mistake.”</ORIGINAL_TEXT>
        <TOKEN id="token-71-0" pos="word" morph="none" start_char="2827" end_char="2831">could</TOKEN>
        <TOKEN id="token-71-1" pos="word" morph="none" start_char="2833" end_char="2837">claim</TOKEN>
        <TOKEN id="token-71-2" pos="word" morph="none" start_char="2839" end_char="2840">to</TOKEN>
        <TOKEN id="token-71-3" pos="word" morph="none" start_char="2842" end_char="2845">have</TOKEN>
        <TOKEN id="token-71-4" pos="punct" morph="none" start_char="2847" end_char="2847">“</TOKEN>
        <TOKEN id="token-71-5" pos="word" morph="none" start_char="2848" end_char="2851">made</TOKEN>
        <TOKEN id="token-71-6" pos="word" morph="none" start_char="2853" end_char="2853">a</TOKEN>
        <TOKEN id="token-71-7" pos="word" morph="none" start_char="2855" end_char="2858">good</TOKEN>
        <TOKEN id="token-71-8" pos="word" morph="none" start_char="2860" end_char="2864">faith</TOKEN>
        <TOKEN id="token-71-9" pos="word" morph="none" start_char="2866" end_char="2872">mistake</TOKEN>
        <TOKEN id="token-71-10" pos="punct" morph="none" start_char="2873" end_char="2874">.”</TOKEN>
      </SEG>
      <SEG id="segment-72" start_char="2876" end_char="2879">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-72-0" pos="unknown" morph="none" start_char="2876" end_char="2879">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-73" start_char="2881" end_char="2883">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-73-0" pos="unknown" morph="none" start_char="2881" end_char="2883">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-74" start_char="2885" end_char="2953">
        <ORIGINAL_TEXT>Further, Saadia said that both the minuscule funds and the actions in</ORIGINAL_TEXT>
        <TOKEN id="token-74-0" pos="word" morph="none" start_char="2885" end_char="2891">Further</TOKEN>
        <TOKEN id="token-74-1" pos="punct" morph="none" start_char="2892" end_char="2892">,</TOKEN>
        <TOKEN id="token-74-2" pos="word" morph="none" start_char="2894" end_char="2899">Saadia</TOKEN>
        <TOKEN id="token-74-3" pos="word" morph="none" start_char="2901" end_char="2904">said</TOKEN>
        <TOKEN id="token-74-4" pos="word" morph="none" start_char="2906" end_char="2909">that</TOKEN>
        <TOKEN id="token-74-5" pos="word" morph="none" start_char="2911" end_char="2914">both</TOKEN>
        <TOKEN id="token-74-6" pos="word" morph="none" start_char="2916" end_char="2918">the</TOKEN>
        <TOKEN id="token-74-7" pos="word" morph="none" start_char="2920" end_char="2928">minuscule</TOKEN>
        <TOKEN id="token-74-8" pos="word" morph="none" start_char="2930" end_char="2934">funds</TOKEN>
        <TOKEN id="token-74-9" pos="word" morph="none" start_char="2936" end_char="2938">and</TOKEN>
        <TOKEN id="token-74-10" pos="word" morph="none" start_char="2940" end_char="2942">the</TOKEN>
        <TOKEN id="token-74-11" pos="word" morph="none" start_char="2944" end_char="2950">actions</TOKEN>
        <TOKEN id="token-74-12" pos="word" morph="none" start_char="2952" end_char="2953">in</TOKEN>
      </SEG>
      <SEG id="segment-75" start_char="2955" end_char="3030">
        <ORIGINAL_TEXT>question, relating essentially to recycling would fall under the category of</ORIGINAL_TEXT>
        <TOKEN id="token-75-0" pos="word" morph="none" start_char="2955" end_char="2962">question</TOKEN>
        <TOKEN id="token-75-1" pos="punct" morph="none" start_char="2963" end_char="2963">,</TOKEN>
        <TOKEN id="token-75-2" pos="word" morph="none" start_char="2965" end_char="2972">relating</TOKEN>
        <TOKEN id="token-75-3" pos="word" morph="none" start_char="2974" end_char="2984">essentially</TOKEN>
        <TOKEN id="token-75-4" pos="word" morph="none" start_char="2986" end_char="2987">to</TOKEN>
        <TOKEN id="token-75-5" pos="word" morph="none" start_char="2989" end_char="2997">recycling</TOKEN>
        <TOKEN id="token-75-6" pos="word" morph="none" start_char="2999" end_char="3003">would</TOKEN>
        <TOKEN id="token-75-7" pos="word" morph="none" start_char="3005" end_char="3008">fall</TOKEN>
        <TOKEN id="token-75-8" pos="word" morph="none" start_char="3010" end_char="3014">under</TOKEN>
        <TOKEN id="token-75-9" pos="word" morph="none" start_char="3016" end_char="3018">the</TOKEN>
        <TOKEN id="token-75-10" pos="word" morph="none" start_char="3020" end_char="3027">category</TOKEN>
        <TOKEN id="token-75-11" pos="word" morph="none" start_char="3029" end_char="3030">of</TOKEN>
      </SEG>
      <SEG id="segment-76" start_char="3032" end_char="3099">
        <ORIGINAL_TEXT>something where “there is no public interest” and of “too minute” an</ORIGINAL_TEXT>
        <TOKEN id="token-76-0" pos="word" morph="none" start_char="3032" end_char="3040">something</TOKEN>
        <TOKEN id="token-76-1" pos="word" morph="none" start_char="3042" end_char="3046">where</TOKEN>
        <TOKEN id="token-76-2" pos="punct" morph="none" start_char="3048" end_char="3048">“</TOKEN>
        <TOKEN id="token-76-3" pos="word" morph="none" start_char="3049" end_char="3053">there</TOKEN>
        <TOKEN id="token-76-4" pos="word" morph="none" start_char="3055" end_char="3056">is</TOKEN>
        <TOKEN id="token-76-5" pos="word" morph="none" start_char="3058" end_char="3059">no</TOKEN>
        <TOKEN id="token-76-6" pos="word" morph="none" start_char="3061" end_char="3066">public</TOKEN>
        <TOKEN id="token-76-7" pos="word" morph="none" start_char="3068" end_char="3075">interest</TOKEN>
        <TOKEN id="token-76-8" pos="punct" morph="none" start_char="3076" end_char="3076">”</TOKEN>
        <TOKEN id="token-76-9" pos="word" morph="none" start_char="3078" end_char="3080">and</TOKEN>
        <TOKEN id="token-76-10" pos="word" morph="none" start_char="3082" end_char="3083">of</TOKEN>
        <TOKEN id="token-76-11" pos="punct" morph="none" start_char="3085" end_char="3085">“</TOKEN>
        <TOKEN id="token-76-12" pos="word" morph="none" start_char="3086" end_char="3088">too</TOKEN>
        <TOKEN id="token-76-13" pos="word" morph="none" start_char="3090" end_char="3095">minute</TOKEN>
        <TOKEN id="token-76-14" pos="punct" morph="none" start_char="3096" end_char="3096">”</TOKEN>
        <TOKEN id="token-76-15" pos="word" morph="none" start_char="3098" end_char="3099">an</TOKEN>
      </SEG>
      <SEG id="segment-77" start_char="3101" end_char="3119">
        <ORIGINAL_TEXT>issue to prosecute.</ORIGINAL_TEXT>
        <TOKEN id="token-77-0" pos="word" morph="none" start_char="3101" end_char="3105">issue</TOKEN>
        <TOKEN id="token-77-1" pos="word" morph="none" start_char="3107" end_char="3108">to</TOKEN>
        <TOKEN id="token-77-2" pos="word" morph="none" start_char="3110" end_char="3118">prosecute</TOKEN>
        <TOKEN id="token-77-3" pos="punct" morph="none" start_char="3119" end_char="3119">.</TOKEN>
      </SEG>
      <SEG id="segment-78" start_char="3121" end_char="3124">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-78-0" pos="unknown" morph="none" start_char="3121" end_char="3124">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-79" start_char="3126" end_char="3128">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-79-0" pos="unknown" morph="none" start_char="3126" end_char="3128">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-80" start_char="3130" end_char="3207">
        <ORIGINAL_TEXT>Dr. Yuval Karniel of the Interdisciplinary Center in Herzliya agreed with much</ORIGINAL_TEXT>
        <TOKEN id="token-80-0" pos="word" morph="none" start_char="3130" end_char="3131">Dr</TOKEN>
        <TOKEN id="token-80-1" pos="punct" morph="none" start_char="3132" end_char="3132">.</TOKEN>
        <TOKEN id="token-80-2" pos="word" morph="none" start_char="3134" end_char="3138">Yuval</TOKEN>
        <TOKEN id="token-80-3" pos="word" morph="none" start_char="3140" end_char="3146">Karniel</TOKEN>
        <TOKEN id="token-80-4" pos="word" morph="none" start_char="3148" end_char="3149">of</TOKEN>
        <TOKEN id="token-80-5" pos="word" morph="none" start_char="3151" end_char="3153">the</TOKEN>
        <TOKEN id="token-80-6" pos="word" morph="none" start_char="3155" end_char="3171">Interdisciplinary</TOKEN>
        <TOKEN id="token-80-7" pos="word" morph="none" start_char="3173" end_char="3178">Center</TOKEN>
        <TOKEN id="token-80-8" pos="word" morph="none" start_char="3180" end_char="3181">in</TOKEN>
        <TOKEN id="token-80-9" pos="word" morph="none" start_char="3183" end_char="3190">Herzliya</TOKEN>
        <TOKEN id="token-80-10" pos="word" morph="none" start_char="3192" end_char="3197">agreed</TOKEN>
        <TOKEN id="token-80-11" pos="word" morph="none" start_char="3199" end_char="3202">with</TOKEN>
        <TOKEN id="token-80-12" pos="word" morph="none" start_char="3204" end_char="3207">much</TOKEN>
      </SEG>
      <SEG id="segment-81" start_char="3209" end_char="3280">
        <ORIGINAL_TEXT>of the thrust of Saadia’s argument, particularly that the issue was “too</ORIGINAL_TEXT>
        <TOKEN id="token-81-0" pos="word" morph="none" start_char="3209" end_char="3210">of</TOKEN>
        <TOKEN id="token-81-1" pos="word" morph="none" start_char="3212" end_char="3214">the</TOKEN>
        <TOKEN id="token-81-2" pos="word" morph="none" start_char="3216" end_char="3221">thrust</TOKEN>
        <TOKEN id="token-81-3" pos="word" morph="none" start_char="3223" end_char="3224">of</TOKEN>
        <TOKEN id="token-81-4" pos="word" morph="none" start_char="3226" end_char="3231">Saadia</TOKEN>
        <TOKEN id="token-81-5" pos="punct" morph="none" start_char="3232" end_char="3232">’</TOKEN>
        <TOKEN id="token-81-6" pos="word" morph="none" start_char="3233" end_char="3233">s</TOKEN>
        <TOKEN id="token-81-7" pos="word" morph="none" start_char="3235" end_char="3242">argument</TOKEN>
        <TOKEN id="token-81-8" pos="punct" morph="none" start_char="3243" end_char="3243">,</TOKEN>
        <TOKEN id="token-81-9" pos="word" morph="none" start_char="3245" end_char="3256">particularly</TOKEN>
        <TOKEN id="token-81-10" pos="word" morph="none" start_char="3258" end_char="3261">that</TOKEN>
        <TOKEN id="token-81-11" pos="word" morph="none" start_char="3263" end_char="3265">the</TOKEN>
        <TOKEN id="token-81-12" pos="word" morph="none" start_char="3267" end_char="3271">issue</TOKEN>
        <TOKEN id="token-81-13" pos="word" morph="none" start_char="3273" end_char="3275">was</TOKEN>
        <TOKEN id="token-81-14" pos="punct" morph="none" start_char="3277" end_char="3277">“</TOKEN>
        <TOKEN id="token-81-15" pos="word" morph="none" start_char="3278" end_char="3280">too</TOKEN>
      </SEG>
      <SEG id="segment-82" start_char="3282" end_char="3329">
        <ORIGINAL_TEXT>minute” to prosecute and waste the state’s time.</ORIGINAL_TEXT>
        <TOKEN id="token-82-0" pos="word" morph="none" start_char="3282" end_char="3287">minute</TOKEN>
        <TOKEN id="token-82-1" pos="punct" morph="none" start_char="3288" end_char="3288">”</TOKEN>
        <TOKEN id="token-82-2" pos="word" morph="none" start_char="3290" end_char="3291">to</TOKEN>
        <TOKEN id="token-82-3" pos="word" morph="none" start_char="3293" end_char="3301">prosecute</TOKEN>
        <TOKEN id="token-82-4" pos="word" morph="none" start_char="3303" end_char="3305">and</TOKEN>
        <TOKEN id="token-82-5" pos="word" morph="none" start_char="3307" end_char="3311">waste</TOKEN>
        <TOKEN id="token-82-6" pos="word" morph="none" start_char="3313" end_char="3315">the</TOKEN>
        <TOKEN id="token-82-7" pos="word" morph="none" start_char="3317" end_char="3321">state</TOKEN>
        <TOKEN id="token-82-8" pos="punct" morph="none" start_char="3322" end_char="3322">’</TOKEN>
        <TOKEN id="token-82-9" pos="word" morph="none" start_char="3323" end_char="3323">s</TOKEN>
        <TOKEN id="token-82-10" pos="word" morph="none" start_char="3325" end_char="3328">time</TOKEN>
        <TOKEN id="token-82-11" pos="punct" morph="none" start_char="3329" end_char="3329">.</TOKEN>
      </SEG>
      <SEG id="segment-83" start_char="3331" end_char="3334">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-83-0" pos="unknown" morph="none" start_char="3331" end_char="3334">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-84" start_char="3336" end_char="3338">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-84-0" pos="unknown" morph="none" start_char="3336" end_char="3338">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-85" start_char="3340" end_char="3408">
        <ORIGINAL_TEXT>Karniel did say that Weinroth’s arguments that Sara’s status could be</ORIGINAL_TEXT>
        <TOKEN id="token-85-0" pos="word" morph="none" start_char="3340" end_char="3346">Karniel</TOKEN>
        <TOKEN id="token-85-1" pos="word" morph="none" start_char="3348" end_char="3350">did</TOKEN>
        <TOKEN id="token-85-2" pos="word" morph="none" start_char="3352" end_char="3354">say</TOKEN>
        <TOKEN id="token-85-3" pos="word" morph="none" start_char="3356" end_char="3359">that</TOKEN>
        <TOKEN id="token-85-4" pos="word" morph="none" start_char="3361" end_char="3368">Weinroth</TOKEN>
        <TOKEN id="token-85-5" pos="punct" morph="none" start_char="3369" end_char="3369">’</TOKEN>
        <TOKEN id="token-85-6" pos="word" morph="none" start_char="3370" end_char="3370">s</TOKEN>
        <TOKEN id="token-85-7" pos="word" morph="none" start_char="3372" end_char="3380">arguments</TOKEN>
        <TOKEN id="token-85-8" pos="word" morph="none" start_char="3382" end_char="3385">that</TOKEN>
        <TOKEN id="token-85-9" pos="word" morph="none" start_char="3387" end_char="3390">Sara</TOKEN>
        <TOKEN id="token-85-10" pos="punct" morph="none" start_char="3391" end_char="3391">’</TOKEN>
        <TOKEN id="token-85-11" pos="word" morph="none" start_char="3392" end_char="3392">s</TOKEN>
        <TOKEN id="token-85-12" pos="word" morph="none" start_char="3394" end_char="3399">status</TOKEN>
        <TOKEN id="token-85-13" pos="word" morph="none" start_char="3401" end_char="3405">could</TOKEN>
        <TOKEN id="token-85-14" pos="word" morph="none" start_char="3407" end_char="3408">be</TOKEN>
      </SEG>
      <SEG id="segment-86" start_char="3410" end_char="3482">
        <ORIGINAL_TEXT>viewed from the perspective of being a collector of the bottles, like any</ORIGINAL_TEXT>
        <TOKEN id="token-86-0" pos="word" morph="none" start_char="3410" end_char="3415">viewed</TOKEN>
        <TOKEN id="token-86-1" pos="word" morph="none" start_char="3417" end_char="3420">from</TOKEN>
        <TOKEN id="token-86-2" pos="word" morph="none" start_char="3422" end_char="3424">the</TOKEN>
        <TOKEN id="token-86-3" pos="word" morph="none" start_char="3426" end_char="3436">perspective</TOKEN>
        <TOKEN id="token-86-4" pos="word" morph="none" start_char="3438" end_char="3439">of</TOKEN>
        <TOKEN id="token-86-5" pos="word" morph="none" start_char="3441" end_char="3445">being</TOKEN>
        <TOKEN id="token-86-6" pos="word" morph="none" start_char="3447" end_char="3447">a</TOKEN>
        <TOKEN id="token-86-7" pos="word" morph="none" start_char="3449" end_char="3457">collector</TOKEN>
        <TOKEN id="token-86-8" pos="word" morph="none" start_char="3459" end_char="3460">of</TOKEN>
        <TOKEN id="token-86-9" pos="word" morph="none" start_char="3462" end_char="3464">the</TOKEN>
        <TOKEN id="token-86-10" pos="word" morph="none" start_char="3466" end_char="3472">bottles</TOKEN>
        <TOKEN id="token-86-11" pos="punct" morph="none" start_char="3473" end_char="3473">,</TOKEN>
        <TOKEN id="token-86-12" pos="word" morph="none" start_char="3475" end_char="3478">like</TOKEN>
        <TOKEN id="token-86-13" pos="word" morph="none" start_char="3480" end_char="3482">any</TOKEN>
      </SEG>
      <SEG id="segment-87" start_char="3484" end_char="3549">
        <ORIGINAL_TEXT>random bottle found by an individual on the street were “cynical.”</ORIGINAL_TEXT>
        <TOKEN id="token-87-0" pos="word" morph="none" start_char="3484" end_char="3489">random</TOKEN>
        <TOKEN id="token-87-1" pos="word" morph="none" start_char="3491" end_char="3496">bottle</TOKEN>
        <TOKEN id="token-87-2" pos="word" morph="none" start_char="3498" end_char="3502">found</TOKEN>
        <TOKEN id="token-87-3" pos="word" morph="none" start_char="3504" end_char="3505">by</TOKEN>
        <TOKEN id="token-87-4" pos="word" morph="none" start_char="3507" end_char="3508">an</TOKEN>
        <TOKEN id="token-87-5" pos="word" morph="none" start_char="3510" end_char="3519">individual</TOKEN>
        <TOKEN id="token-87-6" pos="word" morph="none" start_char="3521" end_char="3522">on</TOKEN>
        <TOKEN id="token-87-7" pos="word" morph="none" start_char="3524" end_char="3526">the</TOKEN>
        <TOKEN id="token-87-8" pos="word" morph="none" start_char="3528" end_char="3533">street</TOKEN>
        <TOKEN id="token-87-9" pos="word" morph="none" start_char="3535" end_char="3538">were</TOKEN>
        <TOKEN id="token-87-10" pos="punct" morph="none" start_char="3540" end_char="3540">“</TOKEN>
        <TOKEN id="token-87-11" pos="word" morph="none" start_char="3541" end_char="3547">cynical</TOKEN>
        <TOKEN id="token-87-12" pos="punct" morph="none" start_char="3548" end_char="3549">.”</TOKEN>
      </SEG>
      <SEG id="segment-88" start_char="3551" end_char="3554">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-88-0" pos="unknown" morph="none" start_char="3551" end_char="3554">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-89" start_char="3556" end_char="3558">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-89-0" pos="unknown" morph="none" start_char="3556" end_char="3558">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-90" start_char="3560" end_char="3632">
        <ORIGINAL_TEXT>He pointed out that most likely Sara not only used state money to buy the</ORIGINAL_TEXT>
        <TOKEN id="token-90-0" pos="word" morph="none" start_char="3560" end_char="3561">He</TOKEN>
        <TOKEN id="token-90-1" pos="word" morph="none" start_char="3563" end_char="3569">pointed</TOKEN>
        <TOKEN id="token-90-2" pos="word" morph="none" start_char="3571" end_char="3573">out</TOKEN>
        <TOKEN id="token-90-3" pos="word" morph="none" start_char="3575" end_char="3578">that</TOKEN>
        <TOKEN id="token-90-4" pos="word" morph="none" start_char="3580" end_char="3583">most</TOKEN>
        <TOKEN id="token-90-5" pos="word" morph="none" start_char="3585" end_char="3590">likely</TOKEN>
        <TOKEN id="token-90-6" pos="word" morph="none" start_char="3592" end_char="3595">Sara</TOKEN>
        <TOKEN id="token-90-7" pos="word" morph="none" start_char="3597" end_char="3599">not</TOKEN>
        <TOKEN id="token-90-8" pos="word" morph="none" start_char="3601" end_char="3604">only</TOKEN>
        <TOKEN id="token-90-9" pos="word" morph="none" start_char="3606" end_char="3609">used</TOKEN>
        <TOKEN id="token-90-10" pos="word" morph="none" start_char="3611" end_char="3615">state</TOKEN>
        <TOKEN id="token-90-11" pos="word" morph="none" start_char="3617" end_char="3621">money</TOKEN>
        <TOKEN id="token-90-12" pos="word" morph="none" start_char="3623" end_char="3624">to</TOKEN>
        <TOKEN id="token-90-13" pos="word" morph="none" start_char="3626" end_char="3628">buy</TOKEN>
        <TOKEN id="token-90-14" pos="word" morph="none" start_char="3630" end_char="3632">the</TOKEN>
      </SEG>
      <SEG id="segment-91" start_char="3634" end_char="3708">
        <ORIGINAL_TEXT>bottles herself, but that she may not even have personally been involved in</ORIGINAL_TEXT>
        <TOKEN id="token-91-0" pos="word" morph="none" start_char="3634" end_char="3640">bottles</TOKEN>
        <TOKEN id="token-91-1" pos="word" morph="none" start_char="3642" end_char="3648">herself</TOKEN>
        <TOKEN id="token-91-2" pos="punct" morph="none" start_char="3649" end_char="3649">,</TOKEN>
        <TOKEN id="token-91-3" pos="word" morph="none" start_char="3651" end_char="3653">but</TOKEN>
        <TOKEN id="token-91-4" pos="word" morph="none" start_char="3655" end_char="3658">that</TOKEN>
        <TOKEN id="token-91-5" pos="word" morph="none" start_char="3660" end_char="3662">she</TOKEN>
        <TOKEN id="token-91-6" pos="word" morph="none" start_char="3664" end_char="3666">may</TOKEN>
        <TOKEN id="token-91-7" pos="word" morph="none" start_char="3668" end_char="3670">not</TOKEN>
        <TOKEN id="token-91-8" pos="word" morph="none" start_char="3672" end_char="3675">even</TOKEN>
        <TOKEN id="token-91-9" pos="word" morph="none" start_char="3677" end_char="3680">have</TOKEN>
        <TOKEN id="token-91-10" pos="word" morph="none" start_char="3682" end_char="3691">personally</TOKEN>
        <TOKEN id="token-91-11" pos="word" morph="none" start_char="3693" end_char="3696">been</TOKEN>
        <TOKEN id="token-91-12" pos="word" morph="none" start_char="3698" end_char="3705">involved</TOKEN>
        <TOKEN id="token-91-13" pos="word" morph="none" start_char="3707" end_char="3708">in</TOKEN>
      </SEG>
      <SEG id="segment-92" start_char="3710" end_char="3784">
        <ORIGINAL_TEXT>their collection, instead ordering state employees to collect them for her.</ORIGINAL_TEXT>
        <TOKEN id="token-92-0" pos="word" morph="none" start_char="3710" end_char="3714">their</TOKEN>
        <TOKEN id="token-92-1" pos="word" morph="none" start_char="3716" end_char="3725">collection</TOKEN>
        <TOKEN id="token-92-2" pos="punct" morph="none" start_char="3726" end_char="3726">,</TOKEN>
        <TOKEN id="token-92-3" pos="word" morph="none" start_char="3728" end_char="3734">instead</TOKEN>
        <TOKEN id="token-92-4" pos="word" morph="none" start_char="3736" end_char="3743">ordering</TOKEN>
        <TOKEN id="token-92-5" pos="word" morph="none" start_char="3745" end_char="3749">state</TOKEN>
        <TOKEN id="token-92-6" pos="word" morph="none" start_char="3751" end_char="3759">employees</TOKEN>
        <TOKEN id="token-92-7" pos="word" morph="none" start_char="3761" end_char="3762">to</TOKEN>
        <TOKEN id="token-92-8" pos="word" morph="none" start_char="3764" end_char="3770">collect</TOKEN>
        <TOKEN id="token-92-9" pos="word" morph="none" start_char="3772" end_char="3775">them</TOKEN>
        <TOKEN id="token-92-10" pos="word" morph="none" start_char="3777" end_char="3779">for</TOKEN>
        <TOKEN id="token-92-11" pos="word" morph="none" start_char="3781" end_char="3783">her</TOKEN>
        <TOKEN id="token-92-12" pos="punct" morph="none" start_char="3784" end_char="3784">.</TOKEN>
      </SEG>
      <SEG id="segment-93" start_char="3786" end_char="3789">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-93-0" pos="unknown" morph="none" start_char="3786" end_char="3789">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-94" start_char="3791" end_char="3793">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-94-0" pos="unknown" morph="none" start_char="3791" end_char="3793">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-95" start_char="3795" end_char="3872">
        <ORIGINAL_TEXT>In contrast, Karniel said, the part of the law that entitles a collector to be</ORIGINAL_TEXT>
        <TOKEN id="token-95-0" pos="word" morph="none" start_char="3795" end_char="3796">In</TOKEN>
        <TOKEN id="token-95-1" pos="word" morph="none" start_char="3798" end_char="3805">contrast</TOKEN>
        <TOKEN id="token-95-2" pos="punct" morph="none" start_char="3806" end_char="3806">,</TOKEN>
        <TOKEN id="token-95-3" pos="word" morph="none" start_char="3808" end_char="3814">Karniel</TOKEN>
        <TOKEN id="token-95-4" pos="word" morph="none" start_char="3816" end_char="3819">said</TOKEN>
        <TOKEN id="token-95-5" pos="punct" morph="none" start_char="3820" end_char="3820">,</TOKEN>
        <TOKEN id="token-95-6" pos="word" morph="none" start_char="3822" end_char="3824">the</TOKEN>
        <TOKEN id="token-95-7" pos="word" morph="none" start_char="3826" end_char="3829">part</TOKEN>
        <TOKEN id="token-95-8" pos="word" morph="none" start_char="3831" end_char="3832">of</TOKEN>
        <TOKEN id="token-95-9" pos="word" morph="none" start_char="3834" end_char="3836">the</TOKEN>
        <TOKEN id="token-95-10" pos="word" morph="none" start_char="3838" end_char="3840">law</TOKEN>
        <TOKEN id="token-95-11" pos="word" morph="none" start_char="3842" end_char="3845">that</TOKEN>
        <TOKEN id="token-95-12" pos="word" morph="none" start_char="3847" end_char="3854">entitles</TOKEN>
        <TOKEN id="token-95-13" pos="word" morph="none" start_char="3856" end_char="3856">a</TOKEN>
        <TOKEN id="token-95-14" pos="word" morph="none" start_char="3858" end_char="3866">collector</TOKEN>
        <TOKEN id="token-95-15" pos="word" morph="none" start_char="3868" end_char="3869">to</TOKEN>
        <TOKEN id="token-95-16" pos="word" morph="none" start_char="3871" end_char="3872">be</TOKEN>
      </SEG>
      <SEG id="segment-96" start_char="3874" end_char="3947">
        <ORIGINAL_TEXT>rewarded without buying the bottle is designated to encourage recycling of</ORIGINAL_TEXT>
        <TOKEN id="token-96-0" pos="word" morph="none" start_char="3874" end_char="3881">rewarded</TOKEN>
        <TOKEN id="token-96-1" pos="word" morph="none" start_char="3883" end_char="3889">without</TOKEN>
        <TOKEN id="token-96-2" pos="word" morph="none" start_char="3891" end_char="3896">buying</TOKEN>
        <TOKEN id="token-96-3" pos="word" morph="none" start_char="3898" end_char="3900">the</TOKEN>
        <TOKEN id="token-96-4" pos="word" morph="none" start_char="3902" end_char="3907">bottle</TOKEN>
        <TOKEN id="token-96-5" pos="word" morph="none" start_char="3909" end_char="3910">is</TOKEN>
        <TOKEN id="token-96-6" pos="word" morph="none" start_char="3912" end_char="3921">designated</TOKEN>
        <TOKEN id="token-96-7" pos="word" morph="none" start_char="3923" end_char="3924">to</TOKEN>
        <TOKEN id="token-96-8" pos="word" morph="none" start_char="3926" end_char="3934">encourage</TOKEN>
        <TOKEN id="token-96-9" pos="word" morph="none" start_char="3936" end_char="3944">recycling</TOKEN>
        <TOKEN id="token-96-10" pos="word" morph="none" start_char="3946" end_char="3947">of</TOKEN>
      </SEG>
      <SEG id="segment-97" start_char="3949" end_char="4016">
        <ORIGINAL_TEXT>bottles which have already been “left behind” by the original buyer.</ORIGINAL_TEXT>
        <TOKEN id="token-97-0" pos="word" morph="none" start_char="3949" end_char="3955">bottles</TOKEN>
        <TOKEN id="token-97-1" pos="word" morph="none" start_char="3957" end_char="3961">which</TOKEN>
        <TOKEN id="token-97-2" pos="word" morph="none" start_char="3963" end_char="3966">have</TOKEN>
        <TOKEN id="token-97-3" pos="word" morph="none" start_char="3968" end_char="3974">already</TOKEN>
        <TOKEN id="token-97-4" pos="word" morph="none" start_char="3976" end_char="3979">been</TOKEN>
        <TOKEN id="token-97-5" pos="punct" morph="none" start_char="3981" end_char="3981">“</TOKEN>
        <TOKEN id="token-97-6" pos="word" morph="none" start_char="3982" end_char="3985">left</TOKEN>
        <TOKEN id="token-97-7" pos="word" morph="none" start_char="3987" end_char="3992">behind</TOKEN>
        <TOKEN id="token-97-8" pos="punct" morph="none" start_char="3993" end_char="3993">”</TOKEN>
        <TOKEN id="token-97-9" pos="word" morph="none" start_char="3995" end_char="3996">by</TOKEN>
        <TOKEN id="token-97-10" pos="word" morph="none" start_char="3998" end_char="4000">the</TOKEN>
        <TOKEN id="token-97-11" pos="word" morph="none" start_char="4002" end_char="4009">original</TOKEN>
        <TOKEN id="token-97-12" pos="word" morph="none" start_char="4011" end_char="4015">buyer</TOKEN>
        <TOKEN id="token-97-13" pos="punct" morph="none" start_char="4016" end_char="4016">.</TOKEN>
      </SEG>
      <SEG id="segment-98" start_char="4018" end_char="4021">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-98-0" pos="unknown" morph="none" start_char="4018" end_char="4021">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-99" start_char="4023" end_char="4025">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-99-0" pos="unknown" morph="none" start_char="4023" end_char="4025">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-100" start_char="4027" end_char="4104">
        <ORIGINAL_TEXT>If Sara obtained the bottles using state funds, with a later intent to cash in</ORIGINAL_TEXT>
        <TOKEN id="token-100-0" pos="word" morph="none" start_char="4027" end_char="4028">If</TOKEN>
        <TOKEN id="token-100-1" pos="word" morph="none" start_char="4030" end_char="4033">Sara</TOKEN>
        <TOKEN id="token-100-2" pos="word" morph="none" start_char="4035" end_char="4042">obtained</TOKEN>
        <TOKEN id="token-100-3" pos="word" morph="none" start_char="4044" end_char="4046">the</TOKEN>
        <TOKEN id="token-100-4" pos="word" morph="none" start_char="4048" end_char="4054">bottles</TOKEN>
        <TOKEN id="token-100-5" pos="word" morph="none" start_char="4056" end_char="4060">using</TOKEN>
        <TOKEN id="token-100-6" pos="word" morph="none" start_char="4062" end_char="4066">state</TOKEN>
        <TOKEN id="token-100-7" pos="word" morph="none" start_char="4068" end_char="4072">funds</TOKEN>
        <TOKEN id="token-100-8" pos="punct" morph="none" start_char="4073" end_char="4073">,</TOKEN>
        <TOKEN id="token-100-9" pos="word" morph="none" start_char="4075" end_char="4078">with</TOKEN>
        <TOKEN id="token-100-10" pos="word" morph="none" start_char="4080" end_char="4080">a</TOKEN>
        <TOKEN id="token-100-11" pos="word" morph="none" start_char="4082" end_char="4086">later</TOKEN>
        <TOKEN id="token-100-12" pos="word" morph="none" start_char="4088" end_char="4093">intent</TOKEN>
        <TOKEN id="token-100-13" pos="word" morph="none" start_char="4095" end_char="4096">to</TOKEN>
        <TOKEN id="token-100-14" pos="word" morph="none" start_char="4098" end_char="4101">cash</TOKEN>
        <TOKEN id="token-100-15" pos="word" morph="none" start_char="4103" end_char="4104">in</TOKEN>
      </SEG>
      <SEG id="segment-101" start_char="4106" end_char="4177">
        <ORIGINAL_TEXT>on collecting them, she was not in the shoes of “Joe collector,” when he</ORIGINAL_TEXT>
        <TOKEN id="token-101-0" pos="word" morph="none" start_char="4106" end_char="4107">on</TOKEN>
        <TOKEN id="token-101-1" pos="word" morph="none" start_char="4109" end_char="4118">collecting</TOKEN>
        <TOKEN id="token-101-2" pos="word" morph="none" start_char="4120" end_char="4123">them</TOKEN>
        <TOKEN id="token-101-3" pos="punct" morph="none" start_char="4124" end_char="4124">,</TOKEN>
        <TOKEN id="token-101-4" pos="word" morph="none" start_char="4126" end_char="4128">she</TOKEN>
        <TOKEN id="token-101-5" pos="word" morph="none" start_char="4130" end_char="4132">was</TOKEN>
        <TOKEN id="token-101-6" pos="word" morph="none" start_char="4134" end_char="4136">not</TOKEN>
        <TOKEN id="token-101-7" pos="word" morph="none" start_char="4138" end_char="4139">in</TOKEN>
        <TOKEN id="token-101-8" pos="word" morph="none" start_char="4141" end_char="4143">the</TOKEN>
        <TOKEN id="token-101-9" pos="word" morph="none" start_char="4145" end_char="4149">shoes</TOKEN>
        <TOKEN id="token-101-10" pos="word" morph="none" start_char="4151" end_char="4152">of</TOKEN>
        <TOKEN id="token-101-11" pos="punct" morph="none" start_char="4154" end_char="4154">“</TOKEN>
        <TOKEN id="token-101-12" pos="word" morph="none" start_char="4155" end_char="4157">Joe</TOKEN>
        <TOKEN id="token-101-13" pos="word" morph="none" start_char="4159" end_char="4167">collector</TOKEN>
        <TOKEN id="token-101-14" pos="punct" morph="none" start_char="4168" end_char="4169">,”</TOKEN>
        <TOKEN id="token-101-15" pos="word" morph="none" start_char="4171" end_char="4174">when</TOKEN>
        <TOKEN id="token-101-16" pos="word" morph="none" start_char="4176" end_char="4177">he</TOKEN>
      </SEG>
      <SEG id="segment-102" start_char="4179" end_char="4246">
        <ORIGINAL_TEXT>randomly collects a stray litter-bottle on the street, Karniel said.</ORIGINAL_TEXT>
        <TOKEN id="token-102-0" pos="word" morph="none" start_char="4179" end_char="4186">randomly</TOKEN>
        <TOKEN id="token-102-1" pos="word" morph="none" start_char="4188" end_char="4195">collects</TOKEN>
        <TOKEN id="token-102-2" pos="word" morph="none" start_char="4197" end_char="4197">a</TOKEN>
        <TOKEN id="token-102-3" pos="word" morph="none" start_char="4199" end_char="4203">stray</TOKEN>
        <TOKEN id="token-102-4" pos="word" morph="none" start_char="4205" end_char="4210">litter</TOKEN>
        <TOKEN id="token-102-5" pos="punct" morph="none" start_char="4211" end_char="4211">-</TOKEN>
        <TOKEN id="token-102-6" pos="word" morph="none" start_char="4212" end_char="4217">bottle</TOKEN>
        <TOKEN id="token-102-7" pos="word" morph="none" start_char="4219" end_char="4220">on</TOKEN>
        <TOKEN id="token-102-8" pos="word" morph="none" start_char="4222" end_char="4224">the</TOKEN>
        <TOKEN id="token-102-9" pos="word" morph="none" start_char="4226" end_char="4231">street</TOKEN>
        <TOKEN id="token-102-10" pos="punct" morph="none" start_char="4232" end_char="4232">,</TOKEN>
        <TOKEN id="token-102-11" pos="word" morph="none" start_char="4234" end_char="4240">Karniel</TOKEN>
        <TOKEN id="token-102-12" pos="word" morph="none" start_char="4242" end_char="4245">said</TOKEN>
        <TOKEN id="token-102-13" pos="punct" morph="none" start_char="4246" end_char="4246">.</TOKEN>
      </SEG>
      <SEG id="segment-103" start_char="4248" end_char="4251">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-103-0" pos="unknown" morph="none" start_char="4248" end_char="4251">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-104" start_char="4253" end_char="4255">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-104-0" pos="unknown" morph="none" start_char="4253" end_char="4255">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-105" start_char="4257" end_char="4324">
        <ORIGINAL_TEXT>Still, at the end of the day, Karniel admitted that since the law is</ORIGINAL_TEXT>
        <TOKEN id="token-105-0" pos="word" morph="none" start_char="4257" end_char="4261">Still</TOKEN>
        <TOKEN id="token-105-1" pos="punct" morph="none" start_char="4262" end_char="4262">,</TOKEN>
        <TOKEN id="token-105-2" pos="word" morph="none" start_char="4264" end_char="4265">at</TOKEN>
        <TOKEN id="token-105-3" pos="word" morph="none" start_char="4267" end_char="4269">the</TOKEN>
        <TOKEN id="token-105-4" pos="word" morph="none" start_char="4271" end_char="4273">end</TOKEN>
        <TOKEN id="token-105-5" pos="word" morph="none" start_char="4275" end_char="4276">of</TOKEN>
        <TOKEN id="token-105-6" pos="word" morph="none" start_char="4278" end_char="4280">the</TOKEN>
        <TOKEN id="token-105-7" pos="word" morph="none" start_char="4282" end_char="4284">day</TOKEN>
        <TOKEN id="token-105-8" pos="punct" morph="none" start_char="4285" end_char="4285">,</TOKEN>
        <TOKEN id="token-105-9" pos="word" morph="none" start_char="4287" end_char="4293">Karniel</TOKEN>
        <TOKEN id="token-105-10" pos="word" morph="none" start_char="4295" end_char="4302">admitted</TOKEN>
        <TOKEN id="token-105-11" pos="word" morph="none" start_char="4304" end_char="4307">that</TOKEN>
        <TOKEN id="token-105-12" pos="word" morph="none" start_char="4309" end_char="4313">since</TOKEN>
        <TOKEN id="token-105-13" pos="word" morph="none" start_char="4315" end_char="4317">the</TOKEN>
        <TOKEN id="token-105-14" pos="word" morph="none" start_char="4319" end_char="4321">law</TOKEN>
        <TOKEN id="token-105-15" pos="word" morph="none" start_char="4323" end_char="4324">is</TOKEN>
      </SEG>
      <SEG id="segment-106" start_char="4326" end_char="4402">
        <ORIGINAL_TEXT>relatively new and there are no real clear court decisions on point, it would</ORIGINAL_TEXT>
        <TOKEN id="token-106-0" pos="word" morph="none" start_char="4326" end_char="4335">relatively</TOKEN>
        <TOKEN id="token-106-1" pos="word" morph="none" start_char="4337" end_char="4339">new</TOKEN>
        <TOKEN id="token-106-2" pos="word" morph="none" start_char="4341" end_char="4343">and</TOKEN>
        <TOKEN id="token-106-3" pos="word" morph="none" start_char="4345" end_char="4349">there</TOKEN>
        <TOKEN id="token-106-4" pos="word" morph="none" start_char="4351" end_char="4353">are</TOKEN>
        <TOKEN id="token-106-5" pos="word" morph="none" start_char="4355" end_char="4356">no</TOKEN>
        <TOKEN id="token-106-6" pos="word" morph="none" start_char="4358" end_char="4361">real</TOKEN>
        <TOKEN id="token-106-7" pos="word" morph="none" start_char="4363" end_char="4367">clear</TOKEN>
        <TOKEN id="token-106-8" pos="word" morph="none" start_char="4369" end_char="4373">court</TOKEN>
        <TOKEN id="token-106-9" pos="word" morph="none" start_char="4375" end_char="4383">decisions</TOKEN>
        <TOKEN id="token-106-10" pos="word" morph="none" start_char="4385" end_char="4386">on</TOKEN>
        <TOKEN id="token-106-11" pos="word" morph="none" start_char="4388" end_char="4392">point</TOKEN>
        <TOKEN id="token-106-12" pos="punct" morph="none" start_char="4393" end_char="4393">,</TOKEN>
        <TOKEN id="token-106-13" pos="word" morph="none" start_char="4395" end_char="4396">it</TOKEN>
        <TOKEN id="token-106-14" pos="word" morph="none" start_char="4398" end_char="4402">would</TOKEN>
      </SEG>
      <SEG id="segment-107" start_char="4404" end_char="4481">
        <ORIGINAL_TEXT>be very difficult to argue that Sara should have known that what she was doing</ORIGINAL_TEXT>
        <TOKEN id="token-107-0" pos="word" morph="none" start_char="4404" end_char="4405">be</TOKEN>
        <TOKEN id="token-107-1" pos="word" morph="none" start_char="4407" end_char="4410">very</TOKEN>
        <TOKEN id="token-107-2" pos="word" morph="none" start_char="4412" end_char="4420">difficult</TOKEN>
        <TOKEN id="token-107-3" pos="word" morph="none" start_char="4422" end_char="4423">to</TOKEN>
        <TOKEN id="token-107-4" pos="word" morph="none" start_char="4425" end_char="4429">argue</TOKEN>
        <TOKEN id="token-107-5" pos="word" morph="none" start_char="4431" end_char="4434">that</TOKEN>
        <TOKEN id="token-107-6" pos="word" morph="none" start_char="4436" end_char="4439">Sara</TOKEN>
        <TOKEN id="token-107-7" pos="word" morph="none" start_char="4441" end_char="4446">should</TOKEN>
        <TOKEN id="token-107-8" pos="word" morph="none" start_char="4448" end_char="4451">have</TOKEN>
        <TOKEN id="token-107-9" pos="word" morph="none" start_char="4453" end_char="4457">known</TOKEN>
        <TOKEN id="token-107-10" pos="word" morph="none" start_char="4459" end_char="4462">that</TOKEN>
        <TOKEN id="token-107-11" pos="word" morph="none" start_char="4464" end_char="4467">what</TOKEN>
        <TOKEN id="token-107-12" pos="word" morph="none" start_char="4469" end_char="4471">she</TOKEN>
        <TOKEN id="token-107-13" pos="word" morph="none" start_char="4473" end_char="4475">was</TOKEN>
        <TOKEN id="token-107-14" pos="word" morph="none" start_char="4477" end_char="4481">doing</TOKEN>
      </SEG>
      <SEG id="segment-108" start_char="4483" end_char="4535">
        <ORIGINAL_TEXT>could be viewed as criminal, as opposed to unethical.</ORIGINAL_TEXT>
        <TOKEN id="token-108-0" pos="word" morph="none" start_char="4483" end_char="4487">could</TOKEN>
        <TOKEN id="token-108-1" pos="word" morph="none" start_char="4489" end_char="4490">be</TOKEN>
        <TOKEN id="token-108-2" pos="word" morph="none" start_char="4492" end_char="4497">viewed</TOKEN>
        <TOKEN id="token-108-3" pos="word" morph="none" start_char="4499" end_char="4500">as</TOKEN>
        <TOKEN id="token-108-4" pos="word" morph="none" start_char="4502" end_char="4509">criminal</TOKEN>
        <TOKEN id="token-108-5" pos="punct" morph="none" start_char="4510" end_char="4510">,</TOKEN>
        <TOKEN id="token-108-6" pos="word" morph="none" start_char="4512" end_char="4513">as</TOKEN>
        <TOKEN id="token-108-7" pos="word" morph="none" start_char="4515" end_char="4521">opposed</TOKEN>
        <TOKEN id="token-108-8" pos="word" morph="none" start_char="4523" end_char="4524">to</TOKEN>
        <TOKEN id="token-108-9" pos="word" morph="none" start_char="4526" end_char="4534">unethical</TOKEN>
        <TOKEN id="token-108-10" pos="punct" morph="none" start_char="4535" end_char="4535">.</TOKEN>
      </SEG>
      <SEG id="segment-109" start_char="4537" end_char="4540">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-109-0" pos="unknown" morph="none" start_char="4537" end_char="4540">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-110" start_char="4542" end_char="4544">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-110-0" pos="unknown" morph="none" start_char="4542" end_char="4544">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-111" start_char="4546" end_char="4621">
        <ORIGINAL_TEXT>Since the standard regarding intent for convicting of a crime is so high, it</ORIGINAL_TEXT>
        <TOKEN id="token-111-0" pos="word" morph="none" start_char="4546" end_char="4550">Since</TOKEN>
        <TOKEN id="token-111-1" pos="word" morph="none" start_char="4552" end_char="4554">the</TOKEN>
        <TOKEN id="token-111-2" pos="word" morph="none" start_char="4556" end_char="4563">standard</TOKEN>
        <TOKEN id="token-111-3" pos="word" morph="none" start_char="4565" end_char="4573">regarding</TOKEN>
        <TOKEN id="token-111-4" pos="word" morph="none" start_char="4575" end_char="4580">intent</TOKEN>
        <TOKEN id="token-111-5" pos="word" morph="none" start_char="4582" end_char="4584">for</TOKEN>
        <TOKEN id="token-111-6" pos="word" morph="none" start_char="4586" end_char="4595">convicting</TOKEN>
        <TOKEN id="token-111-7" pos="word" morph="none" start_char="4597" end_char="4598">of</TOKEN>
        <TOKEN id="token-111-8" pos="word" morph="none" start_char="4600" end_char="4600">a</TOKEN>
        <TOKEN id="token-111-9" pos="word" morph="none" start_char="4602" end_char="4606">crime</TOKEN>
        <TOKEN id="token-111-10" pos="word" morph="none" start_char="4608" end_char="4609">is</TOKEN>
        <TOKEN id="token-111-11" pos="word" morph="none" start_char="4611" end_char="4612">so</TOKEN>
        <TOKEN id="token-111-12" pos="word" morph="none" start_char="4614" end_char="4617">high</TOKEN>
        <TOKEN id="token-111-13" pos="punct" morph="none" start_char="4618" end_char="4618">,</TOKEN>
        <TOKEN id="token-111-14" pos="word" morph="none" start_char="4620" end_char="4621">it</TOKEN>
      </SEG>
      <SEG id="segment-112" start_char="4623" end_char="4694">
        <ORIGINAL_TEXT>is hard to imagine filing an indictment against Sara could be justified.</ORIGINAL_TEXT>
        <TOKEN id="token-112-0" pos="word" morph="none" start_char="4623" end_char="4624">is</TOKEN>
        <TOKEN id="token-112-1" pos="word" morph="none" start_char="4626" end_char="4629">hard</TOKEN>
        <TOKEN id="token-112-2" pos="word" morph="none" start_char="4631" end_char="4632">to</TOKEN>
        <TOKEN id="token-112-3" pos="word" morph="none" start_char="4634" end_char="4640">imagine</TOKEN>
        <TOKEN id="token-112-4" pos="word" morph="none" start_char="4642" end_char="4647">filing</TOKEN>
        <TOKEN id="token-112-5" pos="word" morph="none" start_char="4649" end_char="4650">an</TOKEN>
        <TOKEN id="token-112-6" pos="word" morph="none" start_char="4652" end_char="4661">indictment</TOKEN>
        <TOKEN id="token-112-7" pos="word" morph="none" start_char="4663" end_char="4669">against</TOKEN>
        <TOKEN id="token-112-8" pos="word" morph="none" start_char="4671" end_char="4674">Sara</TOKEN>
        <TOKEN id="token-112-9" pos="word" morph="none" start_char="4676" end_char="4680">could</TOKEN>
        <TOKEN id="token-112-10" pos="word" morph="none" start_char="4682" end_char="4683">be</TOKEN>
        <TOKEN id="token-112-11" pos="word" morph="none" start_char="4685" end_char="4693">justified</TOKEN>
        <TOKEN id="token-112-12" pos="punct" morph="none" start_char="4694" end_char="4694">.</TOKEN>
      </SEG>
      <SEG id="segment-113" start_char="4696" end_char="4699">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-113-0" pos="unknown" morph="none" start_char="4696" end_char="4699">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-114" start_char="4701" end_char="4703">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-114-0" pos="unknown" morph="none" start_char="4701" end_char="4703">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-115" start_char="4705" end_char="4776">
        <ORIGINAL_TEXT>If anything, Sara’s idea might have been so “creative” that she may have</ORIGINAL_TEXT>
        <TOKEN id="token-115-0" pos="word" morph="none" start_char="4705" end_char="4706">If</TOKEN>
        <TOKEN id="token-115-1" pos="word" morph="none" start_char="4708" end_char="4715">anything</TOKEN>
        <TOKEN id="token-115-2" pos="punct" morph="none" start_char="4716" end_char="4716">,</TOKEN>
        <TOKEN id="token-115-3" pos="word" morph="none" start_char="4718" end_char="4721">Sara</TOKEN>
        <TOKEN id="token-115-4" pos="punct" morph="none" start_char="4722" end_char="4722">’</TOKEN>
        <TOKEN id="token-115-5" pos="word" morph="none" start_char="4723" end_char="4723">s</TOKEN>
        <TOKEN id="token-115-6" pos="word" morph="none" start_char="4725" end_char="4728">idea</TOKEN>
        <TOKEN id="token-115-7" pos="word" morph="none" start_char="4730" end_char="4734">might</TOKEN>
        <TOKEN id="token-115-8" pos="word" morph="none" start_char="4736" end_char="4739">have</TOKEN>
        <TOKEN id="token-115-9" pos="word" morph="none" start_char="4741" end_char="4744">been</TOKEN>
        <TOKEN id="token-115-10" pos="word" morph="none" start_char="4746" end_char="4747">so</TOKEN>
        <TOKEN id="token-115-11" pos="punct" morph="none" start_char="4749" end_char="4749">“</TOKEN>
        <TOKEN id="token-115-12" pos="word" morph="none" start_char="4750" end_char="4757">creative</TOKEN>
        <TOKEN id="token-115-13" pos="punct" morph="none" start_char="4758" end_char="4758">”</TOKEN>
        <TOKEN id="token-115-14" pos="word" morph="none" start_char="4760" end_char="4763">that</TOKEN>
        <TOKEN id="token-115-15" pos="word" morph="none" start_char="4765" end_char="4767">she</TOKEN>
        <TOKEN id="token-115-16" pos="word" morph="none" start_char="4769" end_char="4771">may</TOKEN>
        <TOKEN id="token-115-17" pos="word" morph="none" start_char="4773" end_char="4776">have</TOKEN>
      </SEG>
      <SEG id="segment-116" start_char="4778" end_char="4854">
        <ORIGINAL_TEXT>been the first to try it, and only future people who try the same thing might</ORIGINAL_TEXT>
        <TOKEN id="token-116-0" pos="word" morph="none" start_char="4778" end_char="4781">been</TOKEN>
        <TOKEN id="token-116-1" pos="word" morph="none" start_char="4783" end_char="4785">the</TOKEN>
        <TOKEN id="token-116-2" pos="word" morph="none" start_char="4787" end_char="4791">first</TOKEN>
        <TOKEN id="token-116-3" pos="word" morph="none" start_char="4793" end_char="4794">to</TOKEN>
        <TOKEN id="token-116-4" pos="word" morph="none" start_char="4796" end_char="4798">try</TOKEN>
        <TOKEN id="token-116-5" pos="word" morph="none" start_char="4800" end_char="4801">it</TOKEN>
        <TOKEN id="token-116-6" pos="punct" morph="none" start_char="4802" end_char="4802">,</TOKEN>
        <TOKEN id="token-116-7" pos="word" morph="none" start_char="4804" end_char="4806">and</TOKEN>
        <TOKEN id="token-116-8" pos="word" morph="none" start_char="4808" end_char="4811">only</TOKEN>
        <TOKEN id="token-116-9" pos="word" morph="none" start_char="4813" end_char="4818">future</TOKEN>
        <TOKEN id="token-116-10" pos="word" morph="none" start_char="4820" end_char="4825">people</TOKEN>
        <TOKEN id="token-116-11" pos="word" morph="none" start_char="4827" end_char="4829">who</TOKEN>
        <TOKEN id="token-116-12" pos="word" morph="none" start_char="4831" end_char="4833">try</TOKEN>
        <TOKEN id="token-116-13" pos="word" morph="none" start_char="4835" end_char="4837">the</TOKEN>
        <TOKEN id="token-116-14" pos="word" morph="none" start_char="4839" end_char="4842">same</TOKEN>
        <TOKEN id="token-116-15" pos="word" morph="none" start_char="4844" end_char="4848">thing</TOKEN>
        <TOKEN id="token-116-16" pos="word" morph="none" start_char="4850" end_char="4854">might</TOKEN>
      </SEG>
      <SEG id="segment-117" start_char="4856" end_char="4927">
        <ORIGINAL_TEXT>be able to be considered “on notice” of the full criminal possibilities.</ORIGINAL_TEXT>
        <TOKEN id="token-117-0" pos="word" morph="none" start_char="4856" end_char="4857">be</TOKEN>
        <TOKEN id="token-117-1" pos="word" morph="none" start_char="4859" end_char="4862">able</TOKEN>
        <TOKEN id="token-117-2" pos="word" morph="none" start_char="4864" end_char="4865">to</TOKEN>
        <TOKEN id="token-117-3" pos="word" morph="none" start_char="4867" end_char="4868">be</TOKEN>
        <TOKEN id="token-117-4" pos="word" morph="none" start_char="4870" end_char="4879">considered</TOKEN>
        <TOKEN id="token-117-5" pos="punct" morph="none" start_char="4881" end_char="4881">“</TOKEN>
        <TOKEN id="token-117-6" pos="word" morph="none" start_char="4882" end_char="4883">on</TOKEN>
        <TOKEN id="token-117-7" pos="word" morph="none" start_char="4885" end_char="4890">notice</TOKEN>
        <TOKEN id="token-117-8" pos="punct" morph="none" start_char="4891" end_char="4891">”</TOKEN>
        <TOKEN id="token-117-9" pos="word" morph="none" start_char="4893" end_char="4894">of</TOKEN>
        <TOKEN id="token-117-10" pos="word" morph="none" start_char="4896" end_char="4898">the</TOKEN>
        <TOKEN id="token-117-11" pos="word" morph="none" start_char="4900" end_char="4903">full</TOKEN>
        <TOKEN id="token-117-12" pos="word" morph="none" start_char="4905" end_char="4912">criminal</TOKEN>
        <TOKEN id="token-117-13" pos="word" morph="none" start_char="4914" end_char="4926">possibilities</TOKEN>
        <TOKEN id="token-117-14" pos="punct" morph="none" start_char="4927" end_char="4927">.</TOKEN>
      </SEG>
      <SEG id="segment-118" start_char="4929" end_char="4932">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-118-0" pos="unknown" morph="none" start_char="4929" end_char="4932">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-119" start_char="4934" end_char="4936">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-119-0" pos="unknown" morph="none" start_char="4934" end_char="4936">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-120" start_char="4938" end_char="5014">
        <ORIGINAL_TEXT>Saadia has also added that the timing of raising the allegations right before</ORIGINAL_TEXT>
        <TOKEN id="token-120-0" pos="word" morph="none" start_char="4938" end_char="4943">Saadia</TOKEN>
        <TOKEN id="token-120-1" pos="word" morph="none" start_char="4945" end_char="4947">has</TOKEN>
        <TOKEN id="token-120-2" pos="word" morph="none" start_char="4949" end_char="4952">also</TOKEN>
        <TOKEN id="token-120-3" pos="word" morph="none" start_char="4954" end_char="4958">added</TOKEN>
        <TOKEN id="token-120-4" pos="word" morph="none" start_char="4960" end_char="4963">that</TOKEN>
        <TOKEN id="token-120-5" pos="word" morph="none" start_char="4965" end_char="4967">the</TOKEN>
        <TOKEN id="token-120-6" pos="word" morph="none" start_char="4969" end_char="4974">timing</TOKEN>
        <TOKEN id="token-120-7" pos="word" morph="none" start_char="4976" end_char="4977">of</TOKEN>
        <TOKEN id="token-120-8" pos="word" morph="none" start_char="4979" end_char="4985">raising</TOKEN>
        <TOKEN id="token-120-9" pos="word" morph="none" start_char="4987" end_char="4989">the</TOKEN>
        <TOKEN id="token-120-10" pos="word" morph="none" start_char="4991" end_char="5001">allegations</TOKEN>
        <TOKEN id="token-120-11" pos="word" morph="none" start_char="5003" end_char="5007">right</TOKEN>
        <TOKEN id="token-120-12" pos="word" morph="none" start_char="5009" end_char="5014">before</TOKEN>
      </SEG>
      <SEG id="segment-121" start_char="5016" end_char="5091">
        <ORIGINAL_TEXT>elections was problematic and would color any decision to file an indictment</ORIGINAL_TEXT>
        <TOKEN id="token-121-0" pos="word" morph="none" start_char="5016" end_char="5024">elections</TOKEN>
        <TOKEN id="token-121-1" pos="word" morph="none" start_char="5026" end_char="5028">was</TOKEN>
        <TOKEN id="token-121-2" pos="word" morph="none" start_char="5030" end_char="5040">problematic</TOKEN>
        <TOKEN id="token-121-3" pos="word" morph="none" start_char="5042" end_char="5044">and</TOKEN>
        <TOKEN id="token-121-4" pos="word" morph="none" start_char="5046" end_char="5050">would</TOKEN>
        <TOKEN id="token-121-5" pos="word" morph="none" start_char="5052" end_char="5056">color</TOKEN>
        <TOKEN id="token-121-6" pos="word" morph="none" start_char="5058" end_char="5060">any</TOKEN>
        <TOKEN id="token-121-7" pos="word" morph="none" start_char="5062" end_char="5069">decision</TOKEN>
        <TOKEN id="token-121-8" pos="word" morph="none" start_char="5071" end_char="5072">to</TOKEN>
        <TOKEN id="token-121-9" pos="word" morph="none" start_char="5074" end_char="5077">file</TOKEN>
        <TOKEN id="token-121-10" pos="word" morph="none" start_char="5079" end_char="5080">an</TOKEN>
        <TOKEN id="token-121-11" pos="word" morph="none" start_char="5082" end_char="5091">indictment</TOKEN>
      </SEG>
      <SEG id="segment-122" start_char="5093" end_char="5105">
        <ORIGINAL_TEXT>as political.</ORIGINAL_TEXT>
        <TOKEN id="token-122-0" pos="word" morph="none" start_char="5093" end_char="5094">as</TOKEN>
        <TOKEN id="token-122-1" pos="word" morph="none" start_char="5096" end_char="5104">political</TOKEN>
        <TOKEN id="token-122-2" pos="punct" morph="none" start_char="5105" end_char="5105">.</TOKEN>
      </SEG>
      <SEG id="segment-123" start_char="5107" end_char="5110">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-123-0" pos="unknown" morph="none" start_char="5107" end_char="5110">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-124" start_char="5112" end_char="5114">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-124-0" pos="unknown" morph="none" start_char="5112" end_char="5114">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-125" start_char="5116" end_char="5190">
        <ORIGINAL_TEXT>All of the above is heavily reinforced by the fact that several experts who</ORIGINAL_TEXT>
        <TOKEN id="token-125-0" pos="word" morph="none" start_char="5116" end_char="5118">All</TOKEN>
        <TOKEN id="token-125-1" pos="word" morph="none" start_char="5120" end_char="5121">of</TOKEN>
        <TOKEN id="token-125-2" pos="word" morph="none" start_char="5123" end_char="5125">the</TOKEN>
        <TOKEN id="token-125-3" pos="word" morph="none" start_char="5127" end_char="5131">above</TOKEN>
        <TOKEN id="token-125-4" pos="word" morph="none" start_char="5133" end_char="5134">is</TOKEN>
        <TOKEN id="token-125-5" pos="word" morph="none" start_char="5136" end_char="5142">heavily</TOKEN>
        <TOKEN id="token-125-6" pos="word" morph="none" start_char="5144" end_char="5153">reinforced</TOKEN>
        <TOKEN id="token-125-7" pos="word" morph="none" start_char="5155" end_char="5156">by</TOKEN>
        <TOKEN id="token-125-8" pos="word" morph="none" start_char="5158" end_char="5160">the</TOKEN>
        <TOKEN id="token-125-9" pos="word" morph="none" start_char="5162" end_char="5165">fact</TOKEN>
        <TOKEN id="token-125-10" pos="word" morph="none" start_char="5167" end_char="5170">that</TOKEN>
        <TOKEN id="token-125-11" pos="word" morph="none" start_char="5172" end_char="5178">several</TOKEN>
        <TOKEN id="token-125-12" pos="word" morph="none" start_char="5180" end_char="5186">experts</TOKEN>
        <TOKEN id="token-125-13" pos="word" morph="none" start_char="5188" end_char="5190">who</TOKEN>
      </SEG>
      <SEG id="segment-126" start_char="5192" end_char="5268">
        <ORIGINAL_TEXT>were consulted on the subject said that they viewed the issue as so minor and</ORIGINAL_TEXT>
        <TOKEN id="token-126-0" pos="word" morph="none" start_char="5192" end_char="5195">were</TOKEN>
        <TOKEN id="token-126-1" pos="word" morph="none" start_char="5197" end_char="5205">consulted</TOKEN>
        <TOKEN id="token-126-2" pos="word" morph="none" start_char="5207" end_char="5208">on</TOKEN>
        <TOKEN id="token-126-3" pos="word" morph="none" start_char="5210" end_char="5212">the</TOKEN>
        <TOKEN id="token-126-4" pos="word" morph="none" start_char="5214" end_char="5220">subject</TOKEN>
        <TOKEN id="token-126-5" pos="word" morph="none" start_char="5222" end_char="5225">said</TOKEN>
        <TOKEN id="token-126-6" pos="word" morph="none" start_char="5227" end_char="5230">that</TOKEN>
        <TOKEN id="token-126-7" pos="word" morph="none" start_char="5232" end_char="5235">they</TOKEN>
        <TOKEN id="token-126-8" pos="word" morph="none" start_char="5237" end_char="5242">viewed</TOKEN>
        <TOKEN id="token-126-9" pos="word" morph="none" start_char="5244" end_char="5246">the</TOKEN>
        <TOKEN id="token-126-10" pos="word" morph="none" start_char="5248" end_char="5252">issue</TOKEN>
        <TOKEN id="token-126-11" pos="word" morph="none" start_char="5254" end_char="5255">as</TOKEN>
        <TOKEN id="token-126-12" pos="word" morph="none" start_char="5257" end_char="5258">so</TOKEN>
        <TOKEN id="token-126-13" pos="word" morph="none" start_char="5260" end_char="5264">minor</TOKEN>
        <TOKEN id="token-126-14" pos="word" morph="none" start_char="5266" end_char="5268">and</TOKEN>
      </SEG>
      <SEG id="segment-127" start_char="5270" end_char="5346">
        <ORIGINAL_TEXT>lacking substance, that they were not even interested in being interviewed on</ORIGINAL_TEXT>
        <TOKEN id="token-127-0" pos="word" morph="none" start_char="5270" end_char="5276">lacking</TOKEN>
        <TOKEN id="token-127-1" pos="word" morph="none" start_char="5278" end_char="5286">substance</TOKEN>
        <TOKEN id="token-127-2" pos="punct" morph="none" start_char="5287" end_char="5287">,</TOKEN>
        <TOKEN id="token-127-3" pos="word" morph="none" start_char="5289" end_char="5292">that</TOKEN>
        <TOKEN id="token-127-4" pos="word" morph="none" start_char="5294" end_char="5297">they</TOKEN>
        <TOKEN id="token-127-5" pos="word" morph="none" start_char="5299" end_char="5302">were</TOKEN>
        <TOKEN id="token-127-6" pos="word" morph="none" start_char="5304" end_char="5306">not</TOKEN>
        <TOKEN id="token-127-7" pos="word" morph="none" start_char="5308" end_char="5311">even</TOKEN>
        <TOKEN id="token-127-8" pos="word" morph="none" start_char="5313" end_char="5322">interested</TOKEN>
        <TOKEN id="token-127-9" pos="word" morph="none" start_char="5324" end_char="5325">in</TOKEN>
        <TOKEN id="token-127-10" pos="word" morph="none" start_char="5327" end_char="5331">being</TOKEN>
        <TOKEN id="token-127-11" pos="word" morph="none" start_char="5333" end_char="5343">interviewed</TOKEN>
        <TOKEN id="token-127-12" pos="word" morph="none" start_char="5345" end_char="5346">on</TOKEN>
      </SEG>
      <SEG id="segment-128" start_char="5348" end_char="5350">
        <ORIGINAL_TEXT>it.</ORIGINAL_TEXT>
        <TOKEN id="token-128-0" pos="word" morph="none" start_char="5348" end_char="5349">it</TOKEN>
        <TOKEN id="token-128-1" pos="punct" morph="none" start_char="5350" end_char="5350">.</TOKEN>
      </SEG>
      <SEG id="segment-129" start_char="5352" end_char="5355">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-129-0" pos="unknown" morph="none" start_char="5352" end_char="5355">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-130" start_char="5357" end_char="5359">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-130-0" pos="unknown" morph="none" start_char="5357" end_char="5359">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-131" start_char="5361" end_char="5438">
        <ORIGINAL_TEXT>Finally, while the state has not expressed an opinion, the vibes it is putting</ORIGINAL_TEXT>
        <TOKEN id="token-131-0" pos="word" morph="none" start_char="5361" end_char="5367">Finally</TOKEN>
        <TOKEN id="token-131-1" pos="punct" morph="none" start_char="5368" end_char="5368">,</TOKEN>
        <TOKEN id="token-131-2" pos="word" morph="none" start_char="5370" end_char="5374">while</TOKEN>
        <TOKEN id="token-131-3" pos="word" morph="none" start_char="5376" end_char="5378">the</TOKEN>
        <TOKEN id="token-131-4" pos="word" morph="none" start_char="5380" end_char="5384">state</TOKEN>
        <TOKEN id="token-131-5" pos="word" morph="none" start_char="5386" end_char="5388">has</TOKEN>
        <TOKEN id="token-131-6" pos="word" morph="none" start_char="5390" end_char="5392">not</TOKEN>
        <TOKEN id="token-131-7" pos="word" morph="none" start_char="5394" end_char="5402">expressed</TOKEN>
        <TOKEN id="token-131-8" pos="word" morph="none" start_char="5404" end_char="5405">an</TOKEN>
        <TOKEN id="token-131-9" pos="word" morph="none" start_char="5407" end_char="5413">opinion</TOKEN>
        <TOKEN id="token-131-10" pos="punct" morph="none" start_char="5414" end_char="5414">,</TOKEN>
        <TOKEN id="token-131-11" pos="word" morph="none" start_char="5416" end_char="5418">the</TOKEN>
        <TOKEN id="token-131-12" pos="word" morph="none" start_char="5420" end_char="5424">vibes</TOKEN>
        <TOKEN id="token-131-13" pos="word" morph="none" start_char="5426" end_char="5427">it</TOKEN>
        <TOKEN id="token-131-14" pos="word" morph="none" start_char="5429" end_char="5430">is</TOKEN>
        <TOKEN id="token-131-15" pos="word" morph="none" start_char="5432" end_char="5438">putting</TOKEN>
      </SEG>
      <SEG id="segment-132" start_char="5440" end_char="5495">
        <ORIGINAL_TEXT>out on this particular issue have not been enthusiastic.</ORIGINAL_TEXT>
        <TOKEN id="token-132-0" pos="word" morph="none" start_char="5440" end_char="5442">out</TOKEN>
        <TOKEN id="token-132-1" pos="word" morph="none" start_char="5444" end_char="5445">on</TOKEN>
        <TOKEN id="token-132-2" pos="word" morph="none" start_char="5447" end_char="5450">this</TOKEN>
        <TOKEN id="token-132-3" pos="word" morph="none" start_char="5452" end_char="5461">particular</TOKEN>
        <TOKEN id="token-132-4" pos="word" morph="none" start_char="5463" end_char="5467">issue</TOKEN>
        <TOKEN id="token-132-5" pos="word" morph="none" start_char="5469" end_char="5472">have</TOKEN>
        <TOKEN id="token-132-6" pos="word" morph="none" start_char="5474" end_char="5476">not</TOKEN>
        <TOKEN id="token-132-7" pos="word" morph="none" start_char="5478" end_char="5481">been</TOKEN>
        <TOKEN id="token-132-8" pos="word" morph="none" start_char="5483" end_char="5494">enthusiastic</TOKEN>
        <TOKEN id="token-132-9" pos="punct" morph="none" start_char="5495" end_char="5495">.</TOKEN>
      </SEG>
      <SEG id="segment-133" start_char="5497" end_char="5500">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-133-0" pos="unknown" morph="none" start_char="5497" end_char="5500">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-134" start_char="5502" end_char="5504">
        <ORIGINAL_TEXT>&lt;P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-134-0" pos="unknown" morph="none" start_char="5502" end_char="5504">&lt;P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-135" start_char="5506" end_char="5575">
        <ORIGINAL_TEXT>So maybe Sara did it and no one may like what she might have done, but</ORIGINAL_TEXT>
        <TOKEN id="token-135-0" pos="word" morph="none" start_char="5506" end_char="5507">So</TOKEN>
        <TOKEN id="token-135-1" pos="word" morph="none" start_char="5509" end_char="5513">maybe</TOKEN>
        <TOKEN id="token-135-2" pos="word" morph="none" start_char="5515" end_char="5518">Sara</TOKEN>
        <TOKEN id="token-135-3" pos="word" morph="none" start_char="5520" end_char="5522">did</TOKEN>
        <TOKEN id="token-135-4" pos="word" morph="none" start_char="5524" end_char="5525">it</TOKEN>
        <TOKEN id="token-135-5" pos="word" morph="none" start_char="5527" end_char="5529">and</TOKEN>
        <TOKEN id="token-135-6" pos="word" morph="none" start_char="5531" end_char="5532">no</TOKEN>
        <TOKEN id="token-135-7" pos="word" morph="none" start_char="5534" end_char="5536">one</TOKEN>
        <TOKEN id="token-135-8" pos="word" morph="none" start_char="5538" end_char="5540">may</TOKEN>
        <TOKEN id="token-135-9" pos="word" morph="none" start_char="5542" end_char="5545">like</TOKEN>
        <TOKEN id="token-135-10" pos="word" morph="none" start_char="5547" end_char="5550">what</TOKEN>
        <TOKEN id="token-135-11" pos="word" morph="none" start_char="5552" end_char="5554">she</TOKEN>
        <TOKEN id="token-135-12" pos="word" morph="none" start_char="5556" end_char="5560">might</TOKEN>
        <TOKEN id="token-135-13" pos="word" morph="none" start_char="5562" end_char="5565">have</TOKEN>
        <TOKEN id="token-135-14" pos="word" morph="none" start_char="5567" end_char="5570">done</TOKEN>
        <TOKEN id="token-135-15" pos="punct" morph="none" start_char="5571" end_char="5571">,</TOKEN>
        <TOKEN id="token-135-16" pos="word" morph="none" start_char="5573" end_char="5575">but</TOKEN>
      </SEG>
      <SEG id="segment-136" start_char="5577" end_char="5630">
        <ORIGINAL_TEXT>legally, this particular allegation may be a dead end.</ORIGINAL_TEXT>
        <TOKEN id="token-136-0" pos="word" morph="none" start_char="5577" end_char="5583">legally</TOKEN>
        <TOKEN id="token-136-1" pos="punct" morph="none" start_char="5584" end_char="5584">,</TOKEN>
        <TOKEN id="token-136-2" pos="word" morph="none" start_char="5586" end_char="5589">this</TOKEN>
        <TOKEN id="token-136-3" pos="word" morph="none" start_char="5591" end_char="5600">particular</TOKEN>
        <TOKEN id="token-136-4" pos="word" morph="none" start_char="5602" end_char="5611">allegation</TOKEN>
        <TOKEN id="token-136-5" pos="word" morph="none" start_char="5613" end_char="5615">may</TOKEN>
        <TOKEN id="token-136-6" pos="word" morph="none" start_char="5617" end_char="5618">be</TOKEN>
        <TOKEN id="token-136-7" pos="word" morph="none" start_char="5620" end_char="5620">a</TOKEN>
        <TOKEN id="token-136-8" pos="word" morph="none" start_char="5622" end_char="5625">dead</TOKEN>
        <TOKEN id="token-136-9" pos="word" morph="none" start_char="5627" end_char="5629">end</TOKEN>
        <TOKEN id="token-136-10" pos="punct" morph="none" start_char="5630" end_char="5630">.</TOKEN>
      </SEG>
      <SEG id="segment-137" start_char="5632" end_char="5635">
        <ORIGINAL_TEXT>&lt;/P&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-137-0" pos="unknown" morph="none" start_char="5632" end_char="5635">&lt;/P&gt;</TOKEN>
      </SEG>
      <SEG id="segment-138" start_char="5637" end_char="5643">
        <ORIGINAL_TEXT>&lt;/TEXT&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-138-0" pos="unknown" morph="none" start_char="5637" end_char="5643">&lt;/TEXT&gt;</TOKEN>
      </SEG>
      <SEG id="segment-139" start_char="5645" end_char="5650">
        <ORIGINAL_TEXT>&lt;/DOC&gt;</ORIGINAL_TEXT>
        <TOKEN id="token-139-0" pos="unknown" morph="none" start_char="5645" end_char="5650">&lt;/DOC&gt;</TOKEN>
      </SEG>
    </TEXT>
  </DOC>
</LCTL_TEXT>
